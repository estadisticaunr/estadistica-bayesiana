---
title: "Pr치ctica - Unidad 2"
nocite: "@Martin2021, @Downey2021, @Johnson2021, @Reich2020"
practica: "Pr치ctica 2"
---

```{r}
#| echo: false
#| include: false

library(ggplot2)
library(dplyr)
library(patchwork)

is_html <- knitr::is_html_output()
options("knitr.graphics.error" = FALSE)
source(here::here("scripts", "utils.R"))
caption_data <- list(
    "tomates" = list(author = "Markus Spiske", handle = "markusspiske", code = "vrbZVyX2k4I")
)
captions <- lapply(caption_data, function(x) do.call(make_unsplash_caption, x))
captions[["monty_hall"]] <- "Las 3 puertas del problema de Monty Hall"
captions[["insectos"]] <- paste0(
    "Una gran variedad de insectos<br>",
    htmltools::a(
        "Im치gen de Freepik", 
        href="https://www.freepik.es/vector-gratis/errores-dibujos-animados-bebe-insecto-divertido-bicho-feliz-linda-mariquita_10722677.htm"
    )
)
```


## Modelos de Distribuciones Conjugadas

Esta secci칩n contiene ejercicios para trabajar con modelos basados en distribuciones
conjugadas. En general, los ejercicios requieren c치lculos o derivaciones que se pueden
realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el 
lenguaje R para verificar los resultados, mostrar soluciones alternativas y ejercitar
el uso de una herramienta que ser치 de suma utilidad a lo largo de todo el curso y de la 
vida profesional.

1.  **쯈ui칠n domina el _posterior_?**

    Para cada una de las situaciones siguientes, se da una distribuci칩n _a priori_ Beta
    para el par치metro $\pi$ de un ensayo binomial. Para cada escenario, identificar cu치l de
    estos se cumple: el _prior_ tiene mayor influencia en el _posterior_, los datos tienen
    m치s influencia en el _posteriori_, o la creencia _a priori_ y los datos influyen de manera
    similar en la creencia _a posteriori_

    i. _Prior_: $\pi \sim \text{Beta}(1,4)$, observaciones: $y=8$ 칠xitos en $n=10$ ensayos.
    i. _Prior_: $\pi \sim \text{Beta}(20,3)$, observaciones: $y=0$ 칠xitos en $n=1$ ensayos.
    i. _Prior_: $\pi \sim \text{Beta}(4,2)$, observaciones: $y=1$ 칠xitos en $n=3$ ensayos.
    i. _Prior_: $\pi \sim \text{Beta}(3,10)$, observaciones: $y=10$ 칠xitos en $n=13$ ensayos.
    i. _Prior_: $\pi \sim \text{Beta}(20,2)$, observaciones: $y=10$ 칠xitos en $n=200$ ensayos.


1.  **M치s o menos certeza**

    Sea $\theta$ la proporci칩n de personas que prefieren los perros a los gatos. Suponga que
    se elige una distribuci칩n $\text{Beta}(7,2)$ para representar la creencia _a priori_

    i. De acuerdo al _prior_ 쯖u치les son valores razonables para $\theta$?
    i. Se observa en una encuesta que $y=19$ de $n=20$ personas prefieren perros, 쯖칩mo cambia
    eso el conocimiento acerca de $\theta$? Comenta en t칠rminos de la evoluci칩n de la credibilidad
    media y del grado de certidumbre acerca de $\theta$.
    i. Si, en lugar de eso, se determina que $y=1$ de $n=20$ personas prefieren perros, 쯖칩mo cambia
    ahora el grado de credibilidad de los diferentes valores de $\theta$?
    i. Si, en lugar de eso, se determina que $y=10$ de $n=20$ personas prefieren perros, 쯖칩mo cambia
    ahora el grado de credibilidad de los diferentes valores de $\theta$?
    
1.  **Pasito a pasito**

    Sea $\theta$ la probabilidad de 칠xito de un evento de inter칠s. Sea $\text{Beta}(2,3)$ la distribuci칩n
    _a priori_ para $\theta$. Actualiza la distribuci칩n _a posteriori_ para $\theta$ secuencialmente:

    i. Primera observaci칩n: 칠xito.
    i. Segunda observaci칩n: 칠xito.
    i. Tercera observaci칩n: fracaso.
    i. Cuarta observaci칩n: 칠xito.
    
1.  **Pasitos tras pasitos**

    Sea $\theta$ la probabilidad de 칠xito de un evento de inter칠s. Sea $\text{Beta}(2,3)$ la distribuci칩n
    _a priori_ para $\theta$. Actualiza la distribuci칩n _a posteriori_ para $\theta$ secuencialmente dados conjuntos
    de cinco observaciones:

    i. Primeras observaciones: tres 칠xitos.
    i. Segundas observaciones: un 칠xito.
    i. Terceras observaciones: un 칠xito.
    i. Cuartas observaci칩nes: dos 칠xitos.
    
1.  **Diferentes observaciones, diferentes _posteriors_**

    Una empresa que fabrica zapatillas est치 dise침ando una publicidad para Instagram. 
    Tres empleados comparten que la creencia _a priori_ para $\pi$, 
    la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, 
    puede expresarse con una distribuci칩n $\text{Beta}(4, 3)$.
    No obstante, los tres empleados realizan tres experimentos distintos y
    por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que
    no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El 칰ltimo
    lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.

    i. Describa el entendimiento _a priori_ que los empleados tienen sobre $\pi$.
    i. Especifique la distribuci칩n _a posteriori_ de cada uno de los empleados.
    i. Compare las distribuciones _a posteriori_ de cada empleado.
    
1. **쮾alletitas o masitas?**

    La UNR re칰ne cada a침o a estudiantes provenientes de diferentes localidades. Cu치ntas cuadras constituyen
    una distancia "caminable" suele ser motivo de discusi칩n, entre otros. Pero la verdadera grieta est치 entre
    la denominaci칩n _galletitas_ versus _masitas_. Un rosarino pone un _prior_ $\text{Beta}(20,2)$ a la proporci칩n
    de personas que dicen _galletitas_, mientras que un oriundo de una localidad del interior dir치 que la credibilidad
    _a priori_ es $\text{Beta}(2,8)$.
    
    i. Resuma ambas distribuciones _a priori_ y explique con sus palabras lo que implican.
    i. Con la informaci칩n de sus compa침eros de curso, actualice ambas distribuciones _a priori_. 
    쮼s suficiente esa informaci칩n para acercar ambas posturas?

1.  游눹 **Mi primera huerta**

    En un campamento de verano para infantes se realizaron actividades
    que promueven el contacto con la naturaleza. Una de las tareas
    consisti칩 en germinar semillas de tomate. Josefina plant칩 18
    semillas en su almaciguera. Al cabo de 5 d칤as, 8 de ellas germinaron.
    Sea $\theta$ la probabilidad de que una semilla de tomate germine 
    y sea $\text{Beta}(1, 1)$ su distribuci칩n _a priori_.

    i. 쯈u칠 informaci칩n implica el _prior_ sobre la probabilidad de germinaci칩n?
    i. Calcule la media y el desv칤o est치ndar _a posteriori_ de $\theta$ a mano.
    i. Verifique el c치lculo utilizando R.
    i. Obtenga un intervalo de credibilidad del 95% para $\theta$.

    ```{r}
    #| echo: false
    #| out-width: 70%
    #| fig-align: center
    #| fig-cap: !expr captions[["tomates"]]
    if (is_html) knitr::include_graphics(file.path("imgs", "tomates.jpg"))
    ```

1.  游눹 **쯈ui칠n dijo que el f칰tbol siempre da revancha?**

    En la final del 2018 de la Copa del Mundo de la FIFA, Francia le gan칩 a Croacia por
    4 a 2. Considere que el n칰mero de goles que un equipo hace en un partido puede modelizarse
    con una distribuci칩n de Poisson. Suponga un par치metro $\lambda_F$ para Francia y uno $\lambda_C$
    para Croacia. Elija una distribuci칩n Gamma _a priori_ para el n칰mero medio de goles por partido
    (es decir, $\lambda_F$ y $\lambda_C$ compartir치n la distribuci칩n _a priori_). $\lambda_F$ da una idea de
    la capacidad de Francia de hacer goles ($\lambda_C$ lo mismo, pero para Croacia).
    
    En funci칩n del resultado del partido, obtenga las distribuciones _a posteriori_ de 
    $\lambda_F$ y $\lambda_C$ y responda utilizando R:

    i. 쯈u칠 probabilidad hay de que Francia fuera un mejor equipo que Croacia? 
    i. Si el mismo partido se jugara de nuevo (cosa que los franceses en aquella 
    oportunidad no pidieron), 쯖u치l es la probabilidad de que Francia ganara de nuevo?
    <!-- http://allendowney.github.io/ThinkBayes2/chap08.html -->
    
    ::: {.callout-tip collapse="true"}

    ## Ayuda
    
    TO DO
    
    :::

1.  **Mir치 si me va a pasar a mi...**

    Durante el desarrollo de las vacunas contra el COVID-19, un medio anunci칩 para una 
    determinada vacuna una eficacia del 100%.

    > En la fase 3 de un ensayo en adolescentes de entre 12 y 15 a침os, la
    > vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostr칩 una
    > eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo
    > cl칤nico involucr칩 2260 j칩venes estadounidenses. En el ensayo, 18 casos
    > de COVID-19 fueron observados en el grupo placebo ($n=1129$) y ninguno
    > en el grupo vacunado ($n=1131$)

    Es de esperar que, en un ensayo m치s grande, aparezca alg칰n caso de COVID-19 en el 
    grupo que recibi칩 el tratamiento. 
    쮺칩mo se estima la probabilidad de algo que a칰n no ocurri칩?
    <!-- http://allendowney.github.io/ThinkBayes2/vaccine2.html -->
    
    ::: {.callout-tip collapse="true"}

    ## Ayuda
    
    TO DO
    
    :::

1.  游눹 **La regla del tres**

    Una estudiante de Licenciatura en Estad칤stica est치 releyendo su
    tesina antes de entregarla. Si en 20 p치ginas encontrase 5 de ellas
    con al menos un _typo_,
    ser칤a _razonable_ estimar que la probabilidad de que una p치gina
    contenga un _typo_ es $\frac{5}{20} = \frac{1}{4}$.
    쯇ero qu칠 ocurre si en 20 p치ginas no encuentra ning칰n error?
    
    Verifcar que, partiendo de un _prior_ uniforme, $\frac{3}{N}$ es una estimaci칩n 
    razonable para $\tau$ (la probabilidad de que una p치gina contenga un _typo_), 
    siendo $N$ el n칰mero de p치ginas. Para ello, grafique la distribuci칩n _a priori_
    que se obtiene al haber observado 0 _typos_ en 10 p치ginas y luego halle 
    la probabilidad de que $\tau < \frac{3}{N}$ para diferentes valores de 
    $N$ (10, 100, 1000, 10000). 
    <!-- https://www.johndcook.com/blog/2010/03/30/statistical-rule-of-three/ -->
    
    ::: {.callout-tip collapse="true"}

    ## Ayuda
    
    TO DO
    
    :::

1.  **쯊en칠s alguien para recomendar?**

    Una colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo
    producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 
    _reviews_. Otro tiene 96% de evaluaciones positivas, con 50 _reviews_. El 칰ltimo tiene
    90% de comentarios positivos, con 200 evaluaciones. 
    쮺u치l de los tres vendedores le recomendar칤as?
    <!-- http://allendowney.github.io/ThinkBayes2/chap18html -->

1.  **Bichos**

    Un bi칩logo quiere determinar la densidad de un insecto en su regi칩n. Su conocimiento _a priori_ del n칰mero
    promedio de insectos por unidad de 치rea ($\text{m}^2$) se puede representar con una distribuci칩n Gamma de 
    media 0.50 y desv칤o est치ndar 0.25. En una investigaci칩n en 20 $\text{m}^2$ de 치rea, se hallan 3, 2, 5, 1 y 2
    insectos en los primeros 5 $\text{m}^2$ y ninguno en la fracci칩n de tierra restante.
    
    i. Halle la distribuci칩n _a posteriori_ del n칰mero medio de insectos por unidad de 치rea.
    i. Halle la distribuci칩n predictiva _a posteriori_ del n칰mero de insectos que se espera encontrar en una
    exploraci칩n de un 치rea de 10 $\text{m}^2$
    
    ```{r}
    #| echo: false
    #| out-width: 70%
    #| fig-align: center
    #| fig-cap: !expr captions[["insectos"]]
    if (is_html) knitr::include_graphics(file.path("imgs", "insectos.jpg"))
    ```

    ::: {.callout-tip collapse="true"}

    ## Ayuda
    
    TO DO
    
    :::

1.  **Alter-ego**

    El profesor Caprista y el profesor Evangetto est치n dando sus primeros cursos de Estad칤stica Bayesiana. 
    Sus colegas les dijeron que el puntaje promedio en un examen final, $\mu$, var칤a normalmente a침o a a침o
    con media 8 y desv칤o est치ndar 0.4. Y adem치s, que los puntajes individuales de lxs estudiantes $Y$ var칤an
    normalmente alrededor de $\mu$ con una desviaci칩n est치ndar de 0.4
    
    i. 쮺u치l es la probabilidad _a priori_ de que un estudiante se saque m치s de 9 en un examen final?
    i. El profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota
    media de 8.6. Halle la distribuci칩n _a posteriori_ de $\mu$.
    i. El profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota
    media de 8.2. Halle la distribuci칩n _a posteriori_ de $\mu$.
    i. Combine las notas de ambos ex치menes para obtener la distribuci칩n _a posteriori_ de $\mu$
    i. 쮺u치l es la probabilidad _a posteriori_ de que un estudiante se saque m치s de 9 en un examen final?


1.  **Inferencia sobre una distribuci칩n de Poisson**

    La distribuci칩n de masa de probabilidad Poisson se define como

    $$
    \begin{array}{lcr}
    \displaystyle p(x \mid \lambda) = \frac{e^{-\lambda}\lambda^x}{x!} & 
    \text{con} &
    x \in \{0, 1, 2, \cdots \}
    \end{array}
    $$

    donde $\lambda > 0$ es la cantidad promedio de veces que ocurre el evento de inter칠s 
    en un periodo o espacio determinado.

    i. Derive el estimador de m치xima verosimilitud del par치metro $\lambda$.
    i. Derive el _posterior_ $p(\lambda \mid \boldsymbol{x})$ suponiendo que el _prior_ sobre 
    $\lambda$ es $\text{Gamma}(\lambda \mid \alpha, \beta)$ con 
    $p(\lambda \mid \alpha, \beta)  \propto \lambda^{\alpha - 1}e ^ {-\lambda \beta}$.  
    Ayuda: El _posterior_ tambi칠n es una distribuci칩n Gamma.
    i. 쮸 qu칠 valor tiende la media a _posteriori_ cuando $\alpha \to 0$ y $\beta \to 0$?  
    Recuerde que la media de una distribuci칩n $\text{Gamma}(\alpha, \beta)$ es $\alpha/\beta$.
    <!-- @Murphy2012 -->
    
    
1.  **El modelo Gamma-Poisson**

    Sea $\lambda$ la tasa de mensajes de WhatsApp que una persona recibe en una hora. 
    Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5
    con desv칤o est치ndar de 0.25 mensajes.
    
    i. Elija una distribuci칩n Gamma que represente adecuadamente lo que se cree acerca de $\lambda$
    i. 쮺u치l es la probabilidad _a priori_ de que la tasa de mensajes sea mayor a 10?
    i. 쮺u치ntos mensajes se espera que reciba una persona en promedio en una hora?
    
    Se sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la 칰ltima hora.
    
    i. Graficar la verosimilitud de $\lambda$.
    i. Determinar la distribuci칩n _a posteriori_ de $\lambda$.
    i. 쮺u치l es la probabilidad _a posteriori_ de que la tasa de mensajes sea mayor a 10? 
    i. 쮺u치ntos mensajes se espera ahora que reciba una persona en promedio en una hora?

1.  游빌 **Inferencia sobre una distribuci칩n Uniforme**

    Considere una distribuci칩n $\text{Uniforme}(0, \theta)$. La funci칩n de densidad de probabilidad es

    $$
    p(x) = \frac{1}{\theta}I(x \in [0, \theta])
    $$

    Sea $\boldsymbol{X} = (X_1,..., X_n)$ un vector de $n$ variables aleatorias independientes
    e id칠nticamente distribuidas seg칰n $p(x)$

    **Inferencia m치ximo-veros칤mil**

    i. 쮺u치l es el estimador m치ximo veros칤mil de $\theta$ (ll치melo $\hat{\theta}_{\text{MV}}$)?
    i. 쯈u칠 probabilidad le asigna el modelo a una nueva observaci칩n $x_{n + 1}$
    usando $\hat{\theta}_{\text{MV}}$?
    i. 쯆bserva alg칰n problema con el resultado anterior? Si es as칤, sugiera una 
    alternativa mejor.

    **Inferencia Bayesiana**

    El _prior_ conjugado de la distribuci칩n uniforme es la distribuci칩n de Pareto.

    Si $X \sim \text{Pareto}(\alpha, m)$, luego

    $$
    p(x| \alpha, m) = \frac{\alpha m^\alpha}{x^{\alpha+1}} \mathbb{I}(x \ge m)
    $$

    Si el _prior_ es una distribuci칩n de Pareto, la distribuci칩n conjunta de $\theta$ y 
    $\boldsymbol{X} = (X_1,..., X_n)$ es 

    $$
    p(\theta, \boldsymbol{X}) 
        = \frac{\alpha m^\alpha}{\theta^{n + \alpha + 1}} 
        \mathbb{I}(\theta \ge \max(\boldsymbol{X}))
    $$

    Llamando $M_x = \text{max}(\boldsymbol{X})$. 
    La evidencia es

    $$
    \begin{aligned}
    p(\boldsymbol{X}) &= \int_{M_x}^\infty
                     \frac{\alpha m^\alpha}{\theta^{n + \alpha + 1}}
                     d\theta \\
    &=  \begin{cases}
        \frac{\alpha}{(n+\alpha)m^n} & \text{Si } M_x \le m \\
        \frac{\alpha m^\alpha}{(n+\alpha)M_x^{n+\alpha}} & \text{Si } M_x > m \\
        \end{cases}
    \end{aligned}
    $$

    Derive el _posterior_ y muestre que puede ser expresado como una distribuci칩n de 
    Pareto.
    <!-- @Murphy2012 -->
    <!-- Hay que mejorar la redaccion y ver que la notacion es consistente -->

1.  游빌 **Inferencia sobre una distribuci칩n Exponencial**

    El tiempo de vida de una m치quina en a침os $X$ es modelado con una distribuci칩n 
    exponencial con par치metro $\theta$ desconocido. La funci칩n de densidad es 

    $$
    \begin{array}{lcrr}
    p(x | \theta) = \theta e^{-\theta x} & \text{con} & x \ge 0, & \theta \ge 0
    \end{array}
    $$

    i. Muestre que el estimador m치ximo veros칤mil (MV) es $\hat{\theta}_\text{MV} = 1/\bar{x}$.
    i. Suponga que se observan los siguientes tiempos de vida de tres m치quinas 
    independientes $x_1 = 5$, $x_2 = 6$, $x_3 = 4$.
    쮺u치l es el valor del estimador MV?
    i. Una experta del 치rea sugiere que $\theta$ debe tener una distribuci칩n _a priori_ 
    que tambi칠n sea exponencial.
    $$
    \begin{aligned}
        \theta \mid \lambda &\sim \text{Exp}(\lambda) \\
        p(\theta \mid \lambda) &= \lambda e^{-\lambda \theta}
    \end{aligned}
    $$
    Elija un valor para el hiperpar치metro $\lambda$ de la distribuci칩n _a priori_ tal que
    $\mathbb{E}(\theta) = 1/3$. Utilice $\lambda_0$ para representar al valor.

    i. 쮺u치l es el _posterior_ $p(\theta \mid \boldsymbol{X}, \lambda_0)$?
    i. 쮼s la distribuci칩n exponencial conjugada con un _likelihood_ exponencial?
    i. Encuentre la media del _posterior_, $\mathbb{E}(\theta \mid \boldsymbol{X}, \lambda_0)$
    i. Explique por que difieren el estimador MV de la media _a posteriori_. 
    쮺u치l es m치s razonable en este ejemplo?
    <!-- @Murphy2012 -->
       
1.  **Otras distribuciones conjugadas (I)**
    
    Considere el siguiente modelo:

    $$
    \begin{array}{l}
    Y\mid\theta \sim \text{Geom}(\theta) \\
    \theta \sim \text{Beta}(\alpha,\beta)
    \end{array}
    $$
    donde la funci칩n de densidad de la distribuci칩n geom칠trica es 
    $p(y\mid\theta) = \theta(1-\theta)^{y-1}$ para $y \in {1,2,\dots}$
    
    i. 쯈u칠 deber칤a ocurrir con la distribuci칩n _a posteriori_ de $\theta$ para poder 
    afirmar que la distribuci칩n geom칠trica es conjugada de la beta?
    i. Derive la distribuci칩n _a posteriori_ de $\theta$ y concluya.

1.  **Otras distribuciones conjugadas (II)**

    Considere el siguiente modelo:
    
    $$
    \begin{aligned}
    Y\mid\theta &\sim \text{BinomialNeg}(\theta, m) \\
    \theta &\sim \text{Beta}(\alpha,\beta)
    \end{aligned}
    $$
    
    donde la funci칩n de densidad de la distribuci칩n binomial negativa es 
    
    $$p(y\mid \theta, m) = {y+m-1 \choose y} \theta^{m} (1-\theta)^y$$
    
    Obtenga la distribuci칩n _a posteriori_ de $\theta$.
    
1.  游빌 **Otras distribuciones conjugadas (III)**

    Considere el siguiente modelo:
    
    $$
    \begin{array}{l}
    Y\mid\theta \sim \text{Exp}(\theta) = \text{Gamma}(1,\theta)
    \end{array}
    $$

    donde la funci칩n de densidad exponencial es $p(y \mid \theta) = \theta e^{-\theta y}$.
    
    Elija una distribuci칩n _a priori_ conjugada de la verosimilitud propuesta y obtenga 
    la expresi칩n para la distribuci칩n de probabilidad _a posteriori_
    <!-- Nota: Es la parametrizacion de la gamma basada en forma y escala (versus forma y _rate_) -->

## 游눹 Simulaciones

A diferencia de la secci칩n anterior, que requiere resolver los ejercicios a mano y 
promueve el uso de la computadora y R de manera complementaria, esta secci칩n 
contiene ejercicios que deben ser resueltos mediante t칠cnicas de simulaci칩n implementadas
en R. Es posible que en algunos casos tambi칠n se pueda obtener una soluci칩n anal칤tica. 
En estos casos, puede resultar de utilidad obtener tambi칠n una soluci칩n a mano para 
validar el resultado, evaluar el nivel de dificultad y ver que tan intuitivo resultan 
ambos enfoques.

1.  **Entrada en calor**

    Para cada una de las siguientes situaciones, hallar los intervalos centrales de credibilidad
    
    i. Intervalo del 95\% para $\pi$ siendo $\pi\mid \boldsymbol{y} \sim \text{Beta}(4,5)$.
    i. Intervalo del 60\% para $\pi$ siendo $\pi\mid \boldsymbol{y} \sim \text{Beta}(4,5)$.
    i. Intervalo del 89\% para $\lambda$ siendo $\lambda\mid \boldsymbol{y} \sim \text{Gamma}(1,8)$.
    i. Intervalo del 95\% para $\lambda$ siendo $\lambda\mid \boldsymbol{y} \sim \text{Gamma}(2,5)$.
    i. Intervalo del 81\% para $\mu$ siendo $\mu\mid \boldsymbol{y} \sim \mathcal{N}(10,2^2)$.
    i. Intervalo del 99\% para $\pi$ siendo $\mu\mid \boldsymbol{y} \sim \mathcal{N}(-3,1^2)$.

1.  **Propiedades frecuentistas de inferencias bayesianas (!!)**
    
    Sea una variable $Y$ tal que $Y \mid \theta \sim \text{Binomial}(n, \theta)$ y 
    $\theta \sim \text{Beta}(1/2, 1/2)$. Mediante un estudio de simulaci칩n calcule la 
    cobertura emp칤rica del intervalo de credibilidad del 95% con $n \in \{1, 5, 10, 25\}$ 
    y $\theta \in \{0.05, 0.10, \dots, 0.50 \}$. Describa las propiedades frecuentistas
    del intervalo de credibilidad bayesiano.
    <!-- @Reich2020 -->

1.  **쯊e preguntaste alguna vez cu치l es la distribuci칩n de un _p-value_?**

    Considere un problema conocido. Se desean comparar dos muestras independientes de 
    tama침o 5 utilizando un test t y utilizando el test de Mann-Whitney.

    i. Considere el caso en que las dos muestras provienen de poblaciones con igual media
    y desv칤o est치ndar (supongamos normal de media nula y varianza unitaria). 
    Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, 
    쯤u칠 distribuci칩n tendr치n los _p-values_ obtenidos para cada test?
    i. Considere ahora el caso en que las dos muestras provienen de poblaciones con 
    diferente media e igual desv칤o est치ndar ($\mathcal{N}(0,1)$ y $\mathcal{N}(1,1)$). 
    Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, 
    쯤u칠 distribuci칩n tendr치n los _p-values_ obtenidos para cada test?

## Elecci칩n de distribuciones _a priori_

Esta 칰tima secci칩n de la pr치ctica tiene como prop칩sito ejercitar el uso de distribuciones
de probabilidad como herramienta para reflejar informaci칩n de un problema determinado.


1.  **Esbozar la distribuci칩n de las siguientes variables**

    i. El n칰mero de personas que compran caf칠 en el bar de la facultad asumiendo 
    distribuci칩n de Poisson.
    i. El peso de perros adultos en kilogramos asumiendo una distribuci칩n Uniforme.
    i. El peso de elefantes adultos en kilogramos asumiendo una distribuci칩n Normal.
    i. El peso de humanos adultos en libras asumiendo una distribuci칩n asim칠trica hacia 
    la derecha.

1.  **Verificar los resultados de manera computacional**  

    Para cada uno cada uno de los ejemplos del ejercicio anterior,
    graficar la distribuci칩n usando R. Seleccionar los par치metros que
    creas razonable, tomar una muestra aleatoria de tama침o 1000 y
    graficar la distribuci칩n en base a las muestras. 쯉e refleja tu
    conocimiento del problema en la distribuci칩n graficada? Si no,
    ajustar los par치metros y repetir el proceso hasta que el resultado
    tenga concuerde con el conocimiento del problema.

1.  游눹 **Hay que amigarse con de la distribuci칩n Beta**

    Comparar las siguientes distribuciones _a priori_.

    - $\text{Beta}(0.5, 0.5)$.
    - $\text{Beta}(1, 1)$.
    - $\text{Beta}(1, 4)$.
    - $\text{Beta}(5, 1.5)$.

    i. 쮼n qu칠 se diferencian?
    i. 쮺u치l de ellas es m치s informativa?
    i. 쮺칩mo lo determinaste?

1.  游눹 **Elicitaci칩n de _priors_**

    En cada una de la situaciones que se describen debajo, ajustar manualmente los 
    par치metros de una distribuci칩n $\text{Beta}$ para que reflejen la informaci칩n brindada.
    No siempre existe una 칰nica respuesta correcta.  

    i. Un amigo se postul칩 para un empleo en LinkedIn y te dijo: "Dir칤a que tengo una 
    chance del 40% de que me den el trabajo, pero no estoy seguro". 
    Cuando le preguntamos un poco mas, dijo que estima sus chances entre un 20% y un 60%.
    i. Un grupo de investigaci칩n del CONICET desarroll칩 una nueva prueba para una 
    enfermedad bastante rara. El grupo espera que esta prueba arroje resultados correctos
    el 80% de las veces, con una varianza de 0.05.
    i. El primo de un amigo es un apasionado de la pesca, lo practica muy seguido, y se 
    dice ser muy bueno. Seg칰n comenta tu amigo, en el asado de los Jueves el pescador dijo
    lo siguiente: 
    
    > Si tengo que hacer un promedio, 9 de cada 10 veces que salgo, vuelvo con algo. 
    > Pero 칰ltimamente te dir칤a que siempre es 10 de 10. Estoy infalible.
    > La verdad es que soy un crack de la pesca.
    
    Ante el descreimiento de algunos de los comensales supo reconocer que no siempre le 
    fue tan bien:
    
    > Tuve mis malas rachas, pero nunca menos de 8 pescas de cada 10 salidas.
    
1.  游눹 **Efecto de la parametrizaci칩n**
    
    Sea $\theta$ la probabilidad de 칠xito en un experimento binomial y sea 
    $\gamma = \frac{\theta}{1-\theta}$ la chance de 칠xito. Utilizar simulaciones para 
    explorar los efectos de las siguientes elecciones de distribuciones _a priori_
    
    i. Si $\theta \sim \text{Uniforme}(0, 1)$, 쯖u치l es el _prior_ inducido para $\gamma$?
    i. Si $\theta \sim \text{Beta}(5, 5)$, 쯖u치l es el _prior_ inducido para $\gamma$?
    i. Si $\gamma \sim \text{Uniforme}(0, 100)$, 쯖u치l es el _prior_ inducido para $\theta$?
    i. Si $\gamma \sim \text{Gamma}(1, 1)$, 쯖u치l es el _prior_ inducido para $\theta$? 
    
    
## Teor칤a de la Decisi칩n

1.  Dada la distribuci칩n _a posteriori_ $p(\theta \mid y)$, probar que el estimador de Bayes
    que minimiza la funci칩n de p칠rdida $L_1$ es la mediana de $p(\theta \mid y)$.
    <!-- https://stats.stackexchange.com/questions/376867/proof-that-posterior-median-is-the-bayes-estimate-of-absolute-loss -->
    
1.  Suponga que la distribuci칩n _a posteriori_ de $\pi$, $p(\pi \mid y)$, es $\text{Beta}(12,4)$.
    Determine mediante simulaci칩n el estimador que minimiza la p칠rdida de Huber:
    $$
    \mathcal{L}(\delta,\pi) =
    \begin{cases}
    \frac{1}{2} (\pi - \delta)^2 \text{ si } |\pi - \delta| \leq \alpha \\
    \alpha \cdot (|\pi - \delta|-\frac{1}{2}\alpha) \text{ en cualquier otro caso}
    \end{cases}
    $$

## Otros

1. 九꽲잺 **Distribuci칩n predictiva a posteriori**

    Considere un modelo $y\mid \theta \sim \mathrm{Bin}(\theta,n)$, donde $\theta$ puede tomar valores discretos $0,\ 0.1,\ 0.2,\ \dots,\ 1$. Se realizaron inferencias sobre $\theta$ y se obtuvo la distribuci칩n _a posteriori_ que se muestra en la parte superior de la @fig-ppd.

    Se desea obtener la distribuci칩n predictiva _a posteriori_ para el n칰mero de 칠xitos $\tilde{y}$ en $n=5$ nuevas realizaciones del experimento. Cada valor de $\theta$ da lugar a una posible distribuci칩n de $\tilde{y}$ de acuerdo a la verosimilitud binomial, como se observa en la parte inferior de la @fig-ppd.
    
```{r}
#| warning: false
#| echo: false
#| fig-width: 12
#| fig-height: 6
#| fig-align: center
#| fig-cap: Distribuci칩n _a posteriori_ y distribuci칩n predictiva _a posteriori_
#| label: fig-ppd

dv <- 
tibble(theta = c(0.3,0.4,0.5,0.6,0.7),
       post = c(0.1,0.2,0.5,0.1,0.1)) |>
  tidyr::crossing(y=0:5) |>
  mutate(prob = dbinom(y,5,theta))

p1 <- 
dv |>
  distinct(theta,post) |>
  ggplot() +
  geom_segment(aes(x=theta,xend=theta,y=0,yend=post),linewidth=2,lineend="round") +
  xlab("풪") +
  ylab("p(풪|y)") +
  scale_x_continuous(limits = c(0,1),breaks = seq(0,1,0.1), expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0,0,0.05)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        plot.margin = unit(c(0.1,0.2,0.1,0.1), "npc"))

p2 <- 
dv |>
  ggplot() +
  geom_segment(aes(x=y,xend=y,y=0,yend=prob), linewidth=1.5, lineend="round") +
  facet_grid(.~paste0("풪 = ",theta)) +
  scale_y_continuous(expand=c(0,0,0,0.05)) +
  xlab(expression(tilde(y))) +
  ylab(expression("p("*tilde(y)*"|"*theta*")")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())

layout <- "
#AAAA#
BBBBBB
"


p1 + p2 + 
  plot_layout(design = layout)
```


  i. Calcule $p(\tilde{y}\mid\theta)$ para cada valor posible de $\theta$ y compruebe que los gr치ficos de la parte inferior de la @fig-ppd son correctos.
  i. Combine los $p(\tilde{y}\mid\theta)$ ponderando por las probabilidades _a posteriori_ de $\theta$, $p(\theta\mid y)$ para obtener la distribuci칩n predictiva _a posteriori_
  i. Compare la varianza de una de las $p(\tilde{y}\mid\theta)$ (por ejemplo, la de $\theta=0.5$) con la varianza de la distribuci칩n predictiva _a posteriori_ 쯤u칠 observa?
  i. A partir de $p(\theta\mid y)$ y de la verosimilitud binomial, obtenga muestras de $p(\tilde{y}\mid y)$ y grafique su distribuci칩n.
    

<!-- 1.  **To Do** Problema de rankear cinco tratamientos. Se tienen los puntajes de diez tratamientos, ${\theta_1,\theta_2,\dots,\theta_10}$.
    Se dan muestras de las distribuciones _a posteriori_, $p(\theta_i\mid y)$. 쮺u치l de los tratamientos es mejor? -->

<!-- 
## Notas

En alg칰n lado podr칤amos tener un glosario, o algo del estilo. El
objetivo es despejar dudas, por ejemplo, sobre las parametrizaciones de
las distribuciones que utilizamos por defecto. Ac치 podriamos mostrar que
usamos $\text{Beta}(a, b)$ con
$\text{pdf}(x) = \frac{\Gamma(a + b)}{\Gamma(a) + \Gamma(b)} x^{a-1}(1-x)^{b-1}$ 


Let $\pi$  be a random variable which can take any value between 0 and 1, i.e.,  
$\pi \in [0,1]$. Then the variability in $\pi$ might be well modeled by a Beta model 
with **shape hyperparameters** $\alpha > 0$ and $\beta > 0$:

$\pi \sim \text{Beta}(\alpha, \beta).$


$$
\begin{equation}
f(\pi) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \pi^{\alpha-1} (1-\pi)^{\beta-1} \;\; \text{ for } \pi \in [0,1] \tag{3.1}
\end{equation}
$$

$\Gamma(z) = \int_0^\infty x^{z-1}e^{-y}dx$ $\Gamma(z + 1) = z \Gamma(z)$

Fun fact: 
when  $z$ is a positive integer, then  $\Gamma(z)$ simplifies to  $\Gamma(z) = (z-1)!$

**Hyperparameter**

A hyperparameter is a parameter used in a prior model.
-->

[
  {
    "objectID": "trabajos_practicos/02_tp2.html",
    "href": "trabajos_practicos/02_tp2.html",
    "title": "TP2: Implementaci√≥n del algoritmo de Metropolis-Hastings",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#metropolis-hastings-en-2d",
    "href": "trabajos_practicos/02_tp2.html#metropolis-hastings-en-2d",
    "title": "TP2: Implementaci√≥n del algoritmo de Metropolis-Hastings",
    "section": "Metropolis-Hastings en 2D",
    "text": "Metropolis-Hastings en 2D\nSe desean tomar muestras de una normal bivariada asim√©trica cuya funci√≥n de densidad viene dada por\n\\[\nf(\\mathbf{x}) = 2\\ \\phi_2(\\mathbf{x}\\mid\\mathbf{0},\\mathbf{\\Omega})\\ \\Phi(\\mathbf{\\alpha}^T\\mathbf{x}) \\qquad \\mathbf{x} \\in \\mathbb{R}^2\n\\]siendo \\(\\phi_2(\\mathbf{x}\\mid\\mathbf{0},\\mathbf{\\Omega})\\) la funci√≥n de densidad de la normal bivariada de media \\(\\mathbf{0}\\) y matriz de covarianza \\(\\mathbf{\\Omega}\\), \\(\\Phi(\\mathbf{\\alpha}^T\\mathbf{x})\\) es la funci√≥n de probabilidad acumulada de la normal est√°ndar \\(\\mathcal{N}(0,1)\\) y \\(\\alpha\\in\\mathbb{R}^2\\) es un vector de par√°metros.\n\nEn este caso, se tiene:\n\\[\n\\mathbf{\\Omega} = \\begin{bmatrix}1.5 & 0.6 \\\\ 0.6 & 1.5 \\end{bmatrix}\n\\] y\n\\[\n\\mathbf{\\alpha} = [2\\quad 0]\n\\]\n\n\n\n\n\nFunci√≥n de densidad de la que se desean obtener muestras\n\n\n\n\n\nEscriba una funci√≥n que implemente el algoritmo de Metropolis-Hastings para tomar muestras de una funci√≥n de probabilidad bivariada dada. Separe en funciones cada una de los pasos del algoritmo. La probabilidad de salto ser√° normal bivariada de matriz de covarianza variable. Otorgue flexibilidad al algoritmo haciendo que reciba como argumento la matriz de covarianza de la probabilidad de transici√≥n.\n\nSe utilizar√° una normal bivariada para proponer un salto en el algoritmo de Metropolis-Hastings. Se explorar√° el efecto de diferentes distribuciones de probabilidad de salto, en funci√≥n de diferentes matrices de covarianza \\(\\mathbf{\\Sigma}\\). Si pensamos en escribir\n\\[\n\\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{bmatrix} \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1\\end{bmatrix}  \\begin{bmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{bmatrix}\n\\]\ncon \\(\\sigma_i\\), la varianza de la componente \\(i\\) y \\(\\rho\\) la correlaci√≥n entre las variables \\(X_1\\) y \\(X_2\\), entonces se deber√°n ensayar diferentes casos: \\(\\sigma_1=\\sigma_2\\) y \\(\\rho = 0\\), \\(\\sigma_1 > \\sigma_2\\) y \\(\\rho=0\\), \\(\\sigma_1<\\sigma_2\\) y \\(\\rho =0\\), \\(\\sigma_1=\\sigma_2\\) y \\(\\rho > 0\\), \\(\\sigma_1=\\sigma_2\\) y \\(\\rho < 0\\). TO DO Hacer gr√°ficos:\n\nPara al menos dos variantes de cada uno de los cinco casos anteriores, comparar las trayectorias seguidas por las cadenas al obtener muestras de \\(f(x)\\)."
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#aplicaci√≥n-tiempo-de-reacci√≥n-humano-proponer-un-prior-para-sigma-y-uno-para-mu",
    "href": "trabajos_practicos/02_tp2.html#aplicaci√≥n-tiempo-de-reacci√≥n-humano-proponer-un-prior-para-sigma-y-uno-para-mu",
    "title": "TP2: Implementaci√≥n del algoritmo de Metropolis-Hastings",
    "section": "Aplicaci√≥n: ¬øtiempo de reacci√≥n humano? ¬øproponer un prior para sigma y uno para mu?",
    "text": "Aplicaci√≥n: ¬øtiempo de reacci√≥n humano? ¬øproponer un prior para sigma y uno para mu?"
  },
  {
    "objectID": "trabajos_practicos/03_tp3.html",
    "href": "trabajos_practicos/03_tp3.html",
    "title": "TP3: Estudio de modelos lineales mediante simulaciones",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "trabajos_practicos/03_tp3.html#simulaciones-y-validaci√≥n-cruzada",
    "href": "trabajos_practicos/03_tp3.html#simulaciones-y-validaci√≥n-cruzada",
    "title": "TP3: Estudio de modelos lineales mediante simulaciones",
    "section": "Simulaciones y validaci√≥n cruzada",
    "text": "Simulaciones y validaci√≥n cruzada\n\nAlgo de calibraci√≥n de probabilidades\nCalcular lppd ‚Äúa mano‚Äù"
  },
  {
    "objectID": "trabajos_practicos/03_tp3.html#enfriamiento-de-agua-en-un-termo",
    "href": "trabajos_practicos/03_tp3.html#enfriamiento-de-agua-en-un-termo",
    "title": "TP3: Estudio de modelos lineales mediante simulaciones",
    "section": "Enfriamiento de agua en un termo",
    "text": "Enfriamiento de agua en un termo\n\nEcuaci√≥n diferencial (verificar)\nTransformaci√≥n de variables\nAjustar el modelo lineal\nEncontrar el posterior del par√°metro transformado\nPredecir la temperatura a la que estar√° despu√©s de un tiempo‚Ä¶"
  },
  {
    "objectID": "trabajos_practicos/descripcion.html",
    "href": "trabajos_practicos/descripcion.html",
    "title": "Generalidades",
    "section": "",
    "text": "Para aprobar la materia es necesario completar tres trabajos pr√°cticos cortos. La denominaci√≥n cortos hace referencia a que los trabajos son guiados y las tareas a realizar est√°n delimitadas.\nLos trabajos pr√°cticos tienen como objetivo repasar y afianzar los conocimientos adquiridos durante las clases, adquirir pr√°ctica en la aplicaci√≥n de conceptos trabajados, mejorar las habilidades de programaci√≥n y el uso de R, e incorporar algunos conceptos complementarios.\nCada trabajo pr√°ctico ser√° presentado y discutido en clase. Se destinar√° una fracci√≥n de la clase a comenzar a pensar algunas de las actividades.\nLa fecha de entrega de cada trabajo pr√°ctico ser√° de dos semanas luego de la fecha de presentaci√≥n. Se podr√° entregar el trabajo pr√°ctico una semana despu√©s de la fecha de entrega con una penalizaci√≥n del 25% de la nota final.\nPara cada trabajo pr√°ctico, cada grupo deber√° entregar un informe en formato pdf donde se resuelvan las actividades propuestas. El informe debe estar obligatoriamente elaborado utilizando \\(\\mathrm{\\LaTeX{}}\\) (a trav√©s de Quarto, RMarkdown o alguna otra variante). Tener en cuenta que los apartados presentados en el enunciado del trabajo pr√°ctico constituyen una gu√≠a de actividades a resolver y no deben responderse uno a uno como si se tratara de un cuestionario. El informe deber√° permitir una lectura fluida de los resultados y an√°lisis presentados. Cuando la resoluci√≥n de una problem√°tica consista en una funci√≥n o porci√≥n de c√≥digo en R, el c√≥digo deber√° mostrarse en el informe.\nSe evaluar√°n los siguientes aspectos del informe: presentaci√≥n, redacci√≥n (claridad, coherencia y cohesi√≥n), est√©tica, resultados obtenidos, profundidad del an√°lisis."
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html",
    "href": "trabajos_practicos/01_tp1.html",
    "title": "TP1: Aplicaci√≥n de modelos conjugados a reviews de Google Maps",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#descubriendo-la-distribuci√≥n-de-dirichlet",
    "href": "trabajos_practicos/01_tp1.html#descubriendo-la-distribuci√≥n-de-dirichlet",
    "title": "TP1: Aplicaci√≥n de modelos conjugados a reviews de Google Maps",
    "section": "Descubriendo la distribuci√≥n de Dirichlet",
    "text": "Descubriendo la distribuci√≥n de Dirichlet\nLa distribuci√≥n Dirichlet es en realidad una familia de distribuciones. Se trata de una familia de distribuciones de probabilidad continuas y multivariadas. La distribuci√≥n de Dirichlet en \\(K \\geq 2\\) es la distribuci√≥n de probabilidad del vector aleatorio \\(X=[X_1, X_2 \\dots, X_K]\\) dimensiones tiene como par√°metro al vector \\(\\mathbf{\\alpha} = [\\alpha_1, \\alpha_2, \\dots,\\alpha_K]\\).\n\\[\nf(x_1,x_2,\\dots,x_k\\mid \\alpha_1, \\alpha_2,\\dots,\\alpha_K) = \\frac{1}{B(\\mathbf{\\alpha})} \\prod_{i=1}^K x_i^{\\alpha_i-1}\n\\]\ndonde los \\(\\alpha_i \\in \\mathbb{R}^+\\) y \\(B(\\mathbf{\\alpha})\\) es la constante que hace que la integral sea unitaria.\nEl soporte de la distribuci√≥n Dirichlet es tal que \\(\\sum_{i=1}^{K} x_i = 1\\) y \\(x_i \\in [0,1]\\). Es decir, los \\(x_i\\) suman 1. Si consideramos, por ejemplo, \\(K=3\\), el vector \\([x_1, x_2, x_3]\\) pertenece al tri√°ngulo en \\(\\mathbb{R}^3\\) que tiene por v√©rtices a los puntos \\((1,0,0)\\), \\((0,1,0)\\) y \\((0,0,1)\\). As√≠, si \\(x_1=1\\) entonces \\(x_2=x_3=0\\).\nM√°s simple a√∫n, cuando \\(K=2\\), el vector \\([x_1,x_2]\\) pertenece al segmento en \\(\\mathbb{R}^2\\) que tiene por extremos a los puntos \\((1,0)\\) y \\((0,1)\\).\nNotar que esta caracter√≠stica hace que los \\(x_1,x_2,\\dots,x_K\\) puedan representar las probabilidades de un experimento con \\(K\\) resultados posibles.\nA t√≠tulo informativo, el soporte de la distribuci√≥n Dirichlet en \\(K\\) dimensiones es lo que se conoce como simplex (est√°ndar) de \\(K-1\\) dimensiones. El \\(3\\)-simplex es un tri√°ngulo y el \\(2\\)-simplex es un segmento. En general, un \\(K-1\\)-simplex es la envolvente convexa de \\(K\\) v√©rtices y, a su vez, es la colecci√≥n de todas las combinaciones convexas de puntos en el conjunto (Teorema de Carath√©odory).\nLos p√°rrafos anteriores, delirantemente matem√°ticos, muestran una virtud particular de la distribuci√≥n Dirichlet de \\(K=3\\) dimensiones. Esta distribuci√≥n, a pesar de ser de una variable aleatoria en \\(\\mathbb{R}^3\\), puede representarse perfectamente de manera gr√°fica en dos dimensiones (aunque utilizando un sistema de coordenadas peculiar: las coordenadas baric√©ntricas), como si se tratara de una distribuci√≥n bivariada.\n\n\n\n\n\nComo los valores posibles del vector aleatorio tridimensional yacen en un plano, la distribuci√≥n de probabilidad puede representarse gr√°ficamente con facilidad\n\n\n\n\n\n\nMostrar que, cuando \\(K=2\\), la distribuci√≥n de Dirichlet es la distribuci√≥n beta de par√°metros \\(a=\\alpha_1\\) y \\(b=\\alpha_2\\)"
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#relaci√≥n-entre-la-distribuci√≥n-dirichlet-y-la-multinomial",
    "href": "trabajos_practicos/01_tp1.html#relaci√≥n-entre-la-distribuci√≥n-dirichlet-y-la-multinomial",
    "title": "TP1: Aplicaci√≥n de modelos conjugados a reviews de Google Maps",
    "section": "Relaci√≥n entre la distribuci√≥n Dirichlet y la multinomial",
    "text": "Relaci√≥n entre la distribuci√≥n Dirichlet y la multinomial\nCuando un experimento puede tener dos resultados posibles, uno de ellos tiene probabilidad \\(p\\) y el otro probabilidad \\(1-p\\). Si coleccionamos \\(N\\) realizaciones del experimento, el n√∫mero de √©xitos es una variable aleatoria con distribuci√≥n binomial. Estudiamos que era natural utilizar la distribuci√≥n beta como distribuci√≥n a priori para \\(p\\), dado que la distribuci√≥n a posteriori¬†tambi√©n era beta.\nAn√°logamente, cuando un experimento puede tener tres resultados posibles, el primero tiene probabilidad \\(p_1\\), el segundo tiene probabilidad \\(p_2\\) y el tercero, probabilidad \\(p_3 = p_1 - p_2\\). Necesariamente debe ser \\(p_1+p_2+p_3=1\\). Si coleccionamos \\(N\\) realizaciones del experimento, el n√∫mero de ocurrencias de cada resultado posible es un vector aleatorio con distribuci√≥n multinomial. Por lo visto hasta aqu√≠, todo parece indicar que, si queremos realizar inferencias sobre \\(p_1\\), \\(p_2\\) y \\(p_3\\), ser√≠a natural utilizar la distribuci√≥n Dirichlet de tres dimensiones como distribuci√≥n a priori para las probabilidades‚Ä¶\nEn efecto, cuando la distribuci√≥n a priori es Dirichlet y la verosimilitud es multinomial, la distribuci√≥n a posteriori tambi√©n es Dirichlet.\n\nSabemos que para una verosimilitud binomial, si la distribuci√≥n a priori de la probabilidad de √©xito \\(p\\) es beta de par√°metros \\(a\\) y \\(b\\) y se observan \\(s\\) √©xitos en \\(N\\) intentos, la distribuci√≥n a posteriori es beta de par√°metros \\(a' = a+s\\) y \\(b'=b+(N-s)\\).\nHallar, por analog√≠a con el caso anterior, los par√°metros de la distribuci√≥n a posteriori que se obtiene si la verosimilitud es multinomial con tres resultados posibles, la distribuci√≥n a priori¬†es Dirichlet de par√°metros \\([\\alpha_1,\\alpha_2,\\alpha_3]\\) y se obtuvieron \\(s_1\\) veces el primer resultado y \\(s_2\\) veces el segundo resultado, sobre un total de \\(N\\) intentos."
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#aplicaci√≥n",
    "href": "trabajos_practicos/01_tp1.html#aplicaci√≥n",
    "title": "TP1: Aplicaci√≥n de modelos conjugados a reviews de Google Maps",
    "section": "Aplicaci√≥n",
    "text": "Aplicaci√≥n\nAsumiremos ahora que las reviews de un local tienen distribuci√≥n multinomial de par√°metros \\([p_1,p_2,\\dots,p_5]\\). Es decir, la probabilidad de que un usuario asigne 1‚≠ê es \\(p_1\\), de que asigne 2‚≠ê es \\(p_2\\), y as√≠ sucesivamente. Llamaremos \\(n_1\\) al n√∫mero de calificaciones de 1‚≠ê, \\(n_2\\) al n√∫mero de calificaciones de 2‚≠ê, y as√≠ sucesivamente. \\(n_1+n_2+n_3+n_4+n_5 = N\\) ser√° el n√∫mero total de reviews.\nSe puede verificar que el n√∫mero esperado de reviews de \\(i\\)‚≠ê ser√° \\(N p_i\\). Por lo tanto, la puntuaci√≥n esperada ser√°:\n\\[\n\\frac{1}{N} (1\\cdot N p_1 + 2\\cdot N p_2 + 3\\cdot N p_3 + 4\\cdot N p_4 +5\\cdot N p_5) = \\sum_{i=1}^5 i\\cdot p_i\n\\]\n\nElija dos combinaciones de posibles valores de \\(p_i\\) que den un valor esperado de 4.1‚≠ê. Piense en una combinaci√≥n que represente acuerdo entre los clientes y otra que indique la presencia de opiniones dispares.\nEscriba una funci√≥n que, dada una combinaci√≥n de valores de \\(p_i\\), simule el proceso de calificaci√≥n de un cliente\nConstruya una funci√≥n que simule la calificaci√≥n de \\(U\\) clientes\nSimule 1000 veces el proceso de 15 clientes que eval√∫an una cafeter√≠a de 4.1‚≠ê y el proceso de 100 clientes que eval√∫an la misma cafeter√≠a. ¬øQu√© se observa?\n\nUtilizaremos este nuevo modelo para realizar inferencias sobre las puntuaciones de Arto y Orlan.\n\nLos datos se generan siguiendo una distribuci√≥n multinomial. ¬øCu√°les son los par√°metros de esa distribuci√≥n multinomial? ¬øQu√© distribuci√≥n a priori ser√≠a conveniente utilizar? ¬øC√≥mo est√° parametrizada esa distribuci√≥n a priori?\nElija los par√°metros de la distribuci√≥n a priori de modo tal que la creencia inicial sea uniforme sobre los posibles valores de los \\(p_i\\). ¬øQu√© implicancias tiene para la puntuaci√≥n en ‚≠ê la distribuci√≥n a priori elegida?\nCon los datos de la introducci√≥n, obtenga la distribuci√≥n a posteriori de los \\(p_i\\) para cada cafeter√≠a.\n¬øCu√°l es la probabilidad de que Orlan sea mejor que Arto?"
  },
  {
    "objectID": "trabajos_practicos/00_tp0.html",
    "href": "trabajos_practicos/00_tp0.html",
    "title": "TP0: Distribuci√≥n de Rocklets azules",
    "section": "",
    "text": "Descargar PDF\n\n\nDistribuci√≥n de Rocklets azules"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Este es el subtitulo‚Ä¶\n\n\n\n ¬† Licenciatura en Estad√≠stica\n ¬† Facultad de Ciencias Econ√≥micas y Estad√≠stica (UNR)\n ¬† 1¬∞ Cuatrimestre 2023"
  },
  {
    "objectID": "index.html#profesores",
    "href": "index.html#profesores",
    "title": "Estad√≠stica Bayesiana",
    "section": "Profesores",
    "text": "Profesores\n\n\nNacho Evangelista\n\n ¬† email_de_nacho@mail.com\n ¬† Mi√©rcoles (7:00-9:00) y Viernes (7:00-9:00)\n\n\n\nTom√°s Capretto\n\n ¬† email_de_tomi@mail.com\n ¬† Lunes (11-13hs) y Mi√©rcoles (7-9hs)"
  },
  {
    "objectID": "practica/practica_01.html",
    "href": "practica/practica_01.html",
    "title": "Pr√°ctica - Unidad 1",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "practica/practica_01.html#regla-de-bayes",
    "href": "practica/practica_01.html#regla-de-bayes",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Regla de Bayes",
    "text": "Regla de Bayes\n\nEscribir la expresi√≥n matem√°tica para cada una de las siguientes descripciones verbales:\n\nProbabilidad de un parametro dados los datos observados\nLa distribucion de probabilidad de los parametros antes de ver los datos\nLa verosimilitud de los datos para un valor dado de los par√°metros\nLa probabilidad de una observaci√≥n nueva dados los datos observados\nLa probabilidad de una observaci√≥n antes de ver los datos\nTo Do Revisar texto. No se si es mejor usar ‚Äúdado/dada‚Äù o ‚Äúcondicional a‚Äù, u otra expresi√≥n.\nTo Do Algunas descripciones no estan cubiertas en la Unidad 1. \n\nEl test infalible\nEn una poblaci√≥n dada, una de cada mil personas tiene una enfermedad. Se toma una persona al azar de la poblaci√≥n, se le aplica un test para detectar dicha enfermedad, y el resultado es positivo. El test se caracteriza por dar positivo el 99% de las veces que una persona tiene la enfermedad. Adem√°s, dicho test tiene una tasa de falsos positivos del 5%.\n\n¬øCu√°l es la probabilidad de que la persona tenga efectivamente la enfermedad?\nSi realizamos el mismo an√°lisis una segunda vez sobre el mismo paciente y obtenemos nuevamente positivo\n\n¬øCu√°l seria la probabilidad que el paciente est√© enfermo?\n¬øY si diera negativo?\n¬øEs el a priori el mismo para el segundo an√°lisis que para el primero? \n\n\n¬øEs verdad que existen los vampiros? Versi√≥n Crep√∫sculo\nEdward quiere probarle a Bella que los vampiros existen. Seg√∫n Bella, hay una probabilidad del 5% de que los vampiros existan. Tambi√©n cree que la probabilidad de que exista alguien con la piel brillante dado que los vampiros existen es del 70%, y que la probabilidad de que alguien tenga la piel brillante si los vampiros no existen es del 3%. Edward lleva a Bella al bosque y le muestra que de hecho su piel brilla como un üíé ¬øCu√°l es la probabilidad que existan los vampiros? \n\n\n\n\n\nRobert Pattinson como Edward en Crep√∫sculo\n\n\n\n\nLos M&Ms azul fueron introducidos en el a√±o 1995 (antes hab√≠a dos tipos de marr√≥n)\n\nAntes de 1995, la mezcla de colores en una bolsa de M&Ms era: 30% marron, 20% amarillo, 20% rojo, 10% verde, 10% naranja y 10% marr√≥n bronceado.\nLuego de 1995, la mezcla pas√≥ a ser: 24% azul, 20% verde, 16% naranja, 14% amarillo, 13% rojo y 13% marr√≥n.\n\nUn amigo tiene dos bolsas de M&M y nos dice que una bolsa es de 1994 y la otra es de 1996, pero no nos dice cu√°l es cu√°l. Nos da un M&M de cada bolsa: uno es amarillo y el otro es verde (ambos posiblemente est√©n vencidos). ¬øCu√°l es la probabilidad de que el amarillo venga de la bolsa de 1994?\n√Årboles enfermos\nUn vivero de la ciudad se destaca por vender una variedad de √°rboles nativos, incluyendo al jacarand√°, ceibo, omb√∫, entre otros. Lamentablemente, el 18% de los √°rboles del vivero estan infectados con moho. Los √°rboles enfremos se componen en un 15% por jacarand√°s, 80% de ceibos, y 5% de otras especies 1. Los √°rboles sanos se componen por un 20% de jacarand√°s, 10% de ceibos, y 70% de otras especies. Con el objetivo de monitorear cuanto se propag√≥ la enfermedad, una de las personas que trabaja en el vivero selecciona al azar uno de los √°rboles para testear.\n\n¬øCu√°l es la probabilidad a priori de que el √°rbolo tenga moho?\nResulta que el √°rbol seleccionado es un ceibo. ¬øCu√°l es la probabilidad de haber seleccionado un ceibo?\n¬øCu√°l es la probabilidad a posteriori de que el ceibo seleccionado tenga moho?\nCompare las probabilidades a priori y a posteriori de que el √°rbol tenga moho. ¬øC√≥mo afecta el an√°lisis el saber que el √°rbol es un ceibo? \n\n\n\n\n\n\nFlor del Ceibo, la flor nacional\n\n\n\n\nTransporte El Impuntual\nUna cierta empresa de transporte regional, que decidimos llamar ‚ÄúEl Impuntual‚Äù, tiene servicios que van desde Rosario hasta Wheelwright varias veces al d√≠a, todos los d√≠as de la semana. Un 30% de los viajes salen a la ma√±ana, otro 30% salen a la tarde, y el restante 40% salen a la noche. Los pasajeros suelen estar muy frustrados ya que un 25% de los viajes salen tarde. De estos viajes demorados, el 40% corresponden a la ma√±ana, un 50% suceden a la tarde, y el 10% restante ocurre a la noche.\nLucio y Franco son dos amigos del pueblo, y se volvieron a sus casas en colectivos diferentes.\n\nLucio se fue en uno de los colectivos de la ma√±ana. ¬øCu√°l es la probabilidad que su viaje est√© demorado?\nEl colectivo de Franco no est√° demorado. ¬øCu√°l es la probabilidad de que est√© viajando en uno de los colectivos de la ma√±ana? \n\n\n\n\n\n\nFoto de Markus Winkler en Unsplash\n\n\n\n\nBeb√© panda\nSupongamos que hay dos especies de osos panda. Ambas especies son igual de frecuentes y viven en la misma regi√≥n. Es m√°s, lucen de la misma forma y comen la misma comida. A√∫n no existe una prueba gen√©tica que pueda diferenciarlos. Lo √∫nico que los diferencia es la cantidad de cr√≠as que suelen tener. Las madres de la especie A dan luz a mellizos el 10% del tiempo. Y las madres de la especie B dan a luz mellizos el 20% del tiempo. En todos los otros casos, estas madres dan a luz un solo beb√© panda.\nUsando un poco la imaginaci√≥n, supongamos que somos la persona encargada de un programa de reproducci√≥n de pandas. Tenemos una panda femenina que acaba de dar a luz a un par de mellizos, pero no sabemos a que especie pertenece.\n\n¬øCu√°l es la probabilidad que la mam√° panda sea de la especie A?\n¬øCu√°l es la probabilidad que vuelva a tener mellizos en la pr√≥xima parici√≥n?\nUn tiempo despu√©s sos encontramos con que en la segunda parici√≥n da a luz a un √∫nico beb√© panda. ¬øCu√°l es la probabilidad de que este panda sea de la especie A? \n\n\n\n\n\n\nFoto de Stone Wang en Unsplash\n\n\n\n\nDemuestra la validez de la siguiente expresi√≥n de la regla de Bayes\n\\[\nP(B_j | A) = \\frac{P(A | B_j) P(B_j)}{\\sum_{k=1}^{K}P(A | B_k) P(B_k)}\n\\]\ndonde \\(A\\) es un evento cualquiera y \\(\\{B_1, \\cdots, B_K\\}\\) forman una partici√≥n. Para ello siga los siguientes pasos\n\nDemuestre que \\(P(B_j | A) P(A) = P(A | B_j) P(B_j)\\).\nDemuestre que \\(P(A) = P(A \\cap B_1) + P(A \\cap \\{\\cup_{k=2}^{K}B_j\\})\\).\nDemuestre que \\(P(A) = \\sum_{k=1}^{K} P(A \\cap B_j)\\).\nJunte las partes para formar la regla de Bayes.  \n\nHouse of Cards\nHay 538 miembros en el Congreso de Estados Unidos. Supongamos que se auditan sus inversiones y se encuentra que 312 de ellos obtuvieron rendimientos por encima del mercado. Asumamos que un miembro honesto del Congreso tiene solo una probabilidad del 50% de tener rendimientos por encima del mercado, pero uno deshonesto que opera con informaci√≥n confidencial tiene una chance del 90% de hacerlo. ¬øCu√°ntos miembros del Congreso son honestos? \nEst√°s a punto de subir a un avi√≥n rumbo a Seattle. Quer√©s saber si ten√©s que llevar un paraguas. Llam√°s a tres amigos que viven en Seattle y les pregunt√°s si est√° lloviendo. Cada uno de ellos tiene una probabilidad de 2/3 de decirte la verdad y 1/3 de mentirte para hacerte una broma. Los tres responden que s√≠ est√° lloviendo. Cu√°l es la probbailidad de que realmente est√© lloviendo en Seattle? Se puede asumir que llueve el 10% del tiempo. \nDos personas dejaron rastros de sangre en la escena del crimen. La sangre de Oliver, un sospechoso, es analizada y resulta ser de tipo ‚Äò0‚Äô. Los rastros de sangre de la escena son de tipo ‚Äò0‚Äô (un tipo com√∫n en la poblaci√≥n, presente en el 60% de las personas) y de tipo ‚ÄòAB‚Äô (un tipo raro, con una frecuencia del 1% en la poblaci√≥n). ¬øEstos datos representan evidencia de que Oliver estaba presente en la escena del crimen? \nNos encontramos con alguien en la calle y nos dice que tiene dos hijxs. Le preguntamos si algunx de ellxs es mujer y nos responde que s√≠. ¬øCu√°l es la probabilidad de que ambxs sean ni√±as? \nElvis Presley ten√≠a un hermano var√≥n que naci√≥ en el mismo parto pero que muri√≥ al poco tiempo. ¬øCu√°l es la probabilidad de que Elvis tuviera un gemelo? Alguna informaci√≥n adicional: en 1935, cuando Elvis naci√≥, 1/3 de los hermanxs del mismo parto eran gemelxs y 2/3 mellizxs; adem√°s, la probabilidad de que dos mellizxs sean del mismo sexo biol√≥gico puede estimarse en 50%, mientras que dos gemelxs son siempre del mismo sexo biol√≥gico. \nDos cajones contienen medias. Uno de ellos tiene igual cantidad de medias blancas y negras. El otro contiene un n√∫mero igual de medias rojas, verdes y azules. Se elige un caj√≥n al azar, se sacan dos medias sin mirar y resultan ser las dos iguales. ¬øCu√°l es la probabilidad de que las medias sean blancas? Sup√≥ngase que sacar la primera media no altera las proporciones. \nProsecutor‚Äôs Fallacy\nSally Clark era una abogada brit√°nica que fue err√≥neamente sentenciada a prisi√≥n perpetua en 1999 por la muerte de sus dos hijos beb√©s. Su hijo mayor, Christopher, muri√≥ con 11 semanas en diciembre de 1996 y su hijo m√°s joven, Harry, con 8 semanas en enero de 1998. Durante el juicio, la defensa argument√≥ que las muertes se debieron al s√≠ndrome de muerte s√∫bita del lactante (SIDS). Clark fue condenada a partir del testimonio del pediatra Sir Roy Meadow, quien argument√≥ en la corte lo siguiente:\n\nEn familias sanas, la chance de muerte por SIDS es de \\(\\frac{1}{8500}\\)\nLa probabilidad de dos muertes por SIDS en la misma familia es aproximadamente \\(\\frac{1}{8500^2} \\approx \\frac{1}{73000000}\\)\nEs, por ende, muy poco probable que Clark sea inocente\n\nLuego de pasar 3 a√±os en priosi√≥n, Clark fue liberada en 2003 luego de que se determinara que el testimonio experto de Meadows era equivocado. Dos mujeres, a las cuales el testimonio de Meadows hab√≠a enviado a prisi√≥n, tambi√©n fueron liberadas.\n\nIdentifica una falla en la probabilidad de \\(\\frac{1}{73000000}\\) dada por Meadows\nIncluso aceptando el n√∫mero anterior como correcto, ¬øcu√°l es el problema de interpretar esa probabilidad como la probabilidad de inocencia de Clark?"
  },
  {
    "objectID": "practica/practica_01.html#popurr√≠",
    "href": "practica/practica_01.html#popurr√≠",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Popurr√≠?",
    "text": "Popurr√≠?\nTodos como que caen dentro de ‚Äúregla de bayes‚Äù. Tenemos que ver como los dividimos\n\nSea \\(X_1 \\sim \\text{Bernoulli}(\\theta)\\) una variable que indica si una especie de √°rboles se halla en un determinado bosque y \\(\\theta \\in [0, 1]\\) representa la probabilidad a priori de que la especie se encuentre en el bosque. Una investigadora selecciona una muestra de \\(n\\) √°rboles del bosque y encuentra que \\(X_2\\) de ellas pertenecen a la especie de inter√©s.\nEl modelo luego es \\[\n\\begin{array}{lc}\nX_2|X_1 \\sim \\text{Binomial}(n, \\lambda X_1) & \\text{con } \\lambda \\in [0, 1]\n\\end{array}\n\\]\n\\(\\lambda\\) representa la probabilidad de detectar la especie, dado que la especie se encuentra en el bosque.\nEncuntre expresiones matem√°ticas en t√©rmino de \\(n\\), \\(\\theta\\) y \\(\\lambda\\) para las siguientes probabilidades:\n\n\\(P(X_1 = 0, X_2 = 0)\\)\n\\(P(X_1 = 0)\\)\n\\(P(X_2 = 0)\\)\n\\(P(X_1 = 0 | X_2 = 0)\\)\n\\(P(X_2 = 0 | X_1 = 0)\\)\n\\(P(X_1 = 0 | X_2 = 1)\\)\n\\(P(X_2 = 0 | X_1 = 1)\\)\nExplique de manera intuitiva c√≥mo es que las probabilidades calculadas en (iv)-(vii) cambian seg√∫n \\(n\\), \\(\\theta\\) y \\(\\lambda\\).\nAsuma \\(\\theta=0.5\\), \\(\\lambda=0.1\\) y \\(X_2 = 0\\) ¬øCu√°n grande debe ser \\(n\\) para que se puede concluir con 95% de confianza que la especie no se encuentra en el bosque? \n\nEn un estudio que utiliza m√©todos de la Estad√≠stica Bayesiana para predecir el n√∫mero de especies que ser√°n descubiertas en el futuro se reporta que la cantidad de especies marinas bivalvas2 descubiertas cada a√±o entre 2010 y 2015 fue 64, 13, 33, 18, 30 y 20.\nSi se representa con \\(Y_t\\) a la cantidad de especies descubierta en el a√±o \\(t\\), y asumiendo:\n\\[\n\\begin{aligned}\nY_t | \\lambda &\\underset{iid}{\\sim} \\text{Poisson}(\\lambda) \\\\\n\\lambda       &\\sim \\text{Uniforme}(0, 100)\n\\end{aligned}\n\\]\nGraficar la distribuci√≥n a posteriori de \\(\\lambda\\). \nSea \\(n\\) la cantidad desconocida de clientes que visitan una tienda en un dia cualquiera. El n√∫mero de clientes que realizan una compra es \\(Y\\) y se cumple que\n\\[\nY | n \\sim \\text{Binomial}(n, \\theta)\n\\]\ndonde \\(\\theta\\) es la probabilidad de compra, dado que se produce la visita a la tienda. La distribuci√≥n a priori de \\(n\\) es \\(n \\sim \\text{Poisson}(5)\\). Bajo el supuesto que \\(\\theta\\) es conocido y que \\(n\\) es desconocido, graficar la distribuci√≥n a posteriori de \\(n\\) para todas las combinaciones de \\(Y \\in \\{0, 5, 10 \\}\\) y \\(\\theta \\in \\{0.2, 0.5\\}\\). Explique cual es del efecto de cambiar \\(Y\\) y \\(\\theta\\) sobre la distribuci√≥n a posteriori.  \nUn amigo arroja un dado y anota en secreto el n√∫mero que sale (llam√©moslo \\(T\\)). A continuaci√≥n, nosotros, con los ojos vendados, arrojamos el dado varias veces. No podemos ver el n√∫mero que sale pero nuestro amigo nos dice si el n√∫mero que sacamos es mayor, menor o igual a \\(T\\).\nSupongamos que nos da la secuencia: \\(G,\\ G,\\ C,\\ I,\\ C,\\ C,\\ C, I,\\ G,\\ C\\) (siendo \\(G\\) m√°s grande, \\(C\\) m√°s chico e \\(I\\) igual). ¬øCu√°l es la distribuci√≥n a posteriori de los valores de \\(T\\)? \nHay dos monedas en una caja. Una de ellas es una moneda com√∫n y la otra es una moneda que tiene dos caras.\n\nSe elige una moneda al azar, se arroja, y se obtiene cara. ¬øCu√°l es la probabilidad de que la moneda elegida sea la falsa?\nSe elige una moneda al azar y se arroja al aire tres veces, obteni√©ndose tres caras. ¬øCu√°l es la probabilidad de que la moneda elegida sea la falsa?\n\nCansada de los experimentos de arrojar una moneda cientos de veces al aire, una estudiante dise√±a un sistema de reconocimiento de im√°genes que determina si sali√≥ cara o ceca y registra el resultado.\nL√≥gicamente, el sistema dise√±ado no es perfecto sino que presenta una tasa de error. En particular, la probabilidad de que clasificar mal es de 0.2 (20% de las veces que sale cara, el sistema dice ceca, y viceversa).\nSe arroja la moneda 250 veces y el sistema detecta 140 caras,\n\n¬øCu√°l es la distribuci√≥n a posteriori de \\(\\theta\\), la probabilidad de obtener cara?\n¬øQu√© ocurre a medida que la probabilidad de clasificar mal var√≠a? \n\nEn las Jornadas Rosarinas de Ciencia de Datos, una expositora est√° dando una charla en un sal√≥n cuando el personal de seguridad la interrumpe porque cree que puede haber m√°s de 1000 personas en la sala, superando el m√°ximo permitido.\nLa expositora piensa que hay menos de 1000 personas y se ofrece a demostrarlo, aunque piensa que contarlas podr√≠a llevar mucho tiempo.\nDecide hacer un experimento:\n\nPregunta cu√°ntas personas nacieron el 11 de mayo. Dos personas levantan la mano.\nPregunta cu√°ntas personas nacieron el 23 de mayo. Una persona levanta la mano.\nPregunta cu√°ntas personas nacieron el 1 de agosto. Nadie levanta la mano.\n\n¬øCu√°ntas personas hay en la sala? O, mejor dicho, ¬øcu√°l es la probabilidad de que haya m√°s de 1000 personas en la sala? \nSupongamos que existe un idioma con seis palabras:\n\\[\n\\text{\\{perro, parra, farra, carro, corro, tarro\\}}\n\\]\nUn an√°lisis ling√º√≠stico exhaustivo de esta lengua ha descubierto que todas las palabras son igualmente probables, excepto por ‚Äòperro‚Äô, que es \\(\\alpha\\) veces m√°s probable que las otras. Adem√°s:\n\nCuando se tipean, un caracter se introduce err√≥neamente con probabilidad \\(\\theta\\);\nTodas las letras tienen la misma probabilidad de producir un error de tipeo;\nSi una letra se tipe√≥ mal, la probabilidad de cometer un error en otro caracter no cambia;\nLos errores son independientes a lo largo de una palabras.\n\n\n¬øCu√°l es la probabilidad de escribir correctamente ‚Äòtarro‚Äô?\n¬øCu√°l es la probabilidad de tipear ‚Äòcerro‚Äô o ‚Äòcurro‚Äô al querer escribir ‚Äòcarro‚Äô?\nUtilizando la Regla de Bayes, desarrollar un corrector gramatical para esta lengua. Para las palabras tipeadas ‚Äòfarra‚Äô, ‚Äòbirra‚Äô y ‚Äòlocos‚Äô, hallar la probabilidad de que cada palabra del diccionario sea la palabra que se hab√≠a querido escribir. Utilizar las siguientes combinaciones de par√°metros:\n\n\\(\\alpha=2\\) y \\(\\theta = 0.1\\)\n\\(\\alpha=50\\) y \\(\\theta = 0.1\\)\n\\(\\alpha=2\\) y \\(\\theta = 0.9\\)"
  },
  {
    "objectID": "practica/practica_01.html#conceptuales",
    "href": "practica/practica_01.html#conceptuales",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Conceptuales",
    "text": "Conceptuales\n\nVoy a conseguir esa pasant√≠a\nLa empresa de tecnolog√≠a en la que todo el mundo quiere trabajar tiene varias vacantes para pasantes en ciencia de datos. Luego de leer la descripci√≥n de la b√∫squeda, te das cuenta que sos una persona calificada para el puesto: estos son tus datos. Tu objetivo es averiguar si te van a ofrecer el puesto: esta es tu hip√≥tesis.\n\nDesde la perspectiva de una persona con un razonamiento frecuentista, ¬øQu√© es lo que se responde al evaluar la hip√≥tesis de que te ofrecen el puesto?\nRepita el punto anterior considerando la perspectiva de una persona con un razonamiento Bayesiano.\n¬øQu√© pregunta tiene m√°s sentido responder: la frecuentista o la Bayesiana? Justifica tu respuesta. \n\nBeneficios de la Estad√≠stica Bayesiana\nUna amiga te cuenta que est√° interesada en aprender m√°s sobre Estad√≠stica Bayesiana. Expl√≠cale lo siguiente:\n\n¬øPor qu√© es √∫til el enfoque Bayesiano?\n¬øCu√°les son las similitudes entre el enfoque frecuentista y el Bayesiano?"
  },
  {
    "objectID": "practica/practica_03.html",
    "href": "practica/practica_03.html",
    "title": "Pr√°ctica - Unidad 3",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "practica/practica_03.html#m√©todos-computacionales",
    "href": "practica/practica_03.html#m√©todos-computacionales",
    "title": "Pr√°ctica - Unidad 3",
    "section": "M√©todos Computacionales",
    "text": "M√©todos Computacionales\n\nEn la Copa del Mundo de la FIFA 2014, Alemania jug√≥ contra Brasil en la semifinal. Los alemanes hicieron el primer gol a los 11 minutos y el segundo a los 23. En ese momento del partido,\n\n¬øCu√°ntos goles cabr√≠a esperar que Alemania hiciera al finalizar los 90 minutos?\n¬øCu√°l era la probabilidad de que Alemania hiciera m√°s de 5 goles (cosa que ocurri√≥)? \n\nGrid approximation para un modelo de tres par√°metros? Una regresi√≥n lineal? No la dimos pero la conocen‚Ä¶"
  },
  {
    "objectID": "practica/practica_00.html",
    "href": "practica/practica_00.html",
    "title": "Pr√°ctica - Unidad 0",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "practica/practica_00.html#probabilidad",
    "href": "practica/practica_00.html#probabilidad",
    "title": "Pr√°ctica - Unidad 0",
    "section": "Probabilidad",
    "text": "Probabilidad\n\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado ‚Äúla probabilidad de que Argentina gane la copa del mundo el 18 de Diciembre de 2022‚Äù?\n\n\\(P(\\text{18 de Diciembre de 2022} | \\text{Argentina campeon})\\)\n\\(P(\\text{Argentina campeon})\\)\n\\(P(\\text{Argentina campeon}, \\text{18 de Diciembre de 2022}) / P(\\text{18 de Diciembre de 2022})\\)\n\\(P(\\text{Argentina campeon} | \\text{Diciembre})\\)\n\\(P(\\text{Argentina campeon} | \\text{18 de Diciembre de 2022})\\) \n\nEnuncie con palabras cada una de las expresiones del punto anterior. \nSeg√∫n la definici√≥n de probabilidad condicional\n\n¬øCu√°l es el valor de \\(P(A | A)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\) en el caso que \\(A\\) y \\(B\\) sean independientes?\nCuando se cumple que \\(P(A | B) = P(A)\\)?\nEs posible que \\(P(A | B) > P(A)\\)? Cuando?\nEs posible que \\(P(A | B) < P(A)\\)? Cuando?"
  },
  {
    "objectID": "practica/practica_00.html#distribuciones",
    "href": "practica/practica_00.html#distribuciones",
    "title": "Pr√°ctica - Unidad 0",
    "section": "Distribuciones",
    "text": "Distribuciones\n\nSea \\(X\\) una variable aleatoria con soporte \\(X \\in \\mathcal{S} = [1, \\infty)\\). Encuentre la constante \\(c\\), en funci√≥n de \\(\\theta\\), que haga que \\(f(x) = c \\exp(-x / \\theta)\\) sea una funci√≥n de densidad de probabilidad (pdf) v√°lida. \nSuponga \\(X \\sim \\text{Uniforme}(a, b)\\), por lo que su soporte es \\(\\mathcal{S} = [a, b]\\) y su funci√≥n de densidad de probabilidad es \\(f(x) = 1 / (b - a)\\) para todo \\(x \\in \\mathcal{S}\\).\n\nPruebe que \\(f(x)\\) es una funci√≥n de densidad de probabilidad v√°lida.\nEncuentre la media y la varianza de \\(X\\). \n\nSeg√∫n los expertos de un problema determinado, se indica que el valor de un par√°metro debe ser positivo y su distribuci√≥n a priori debe tener media igual a 5 y varianza igual a 3. Encuentre una distribuci√≥n que satisfaga estas condiciones. \nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias con funci√≥n de probabilidad conjunta dada por la siguiente tabla.\n\n\n\n\n\n\\(X_1\\) / \\(X_2\\)\n\\(X_2=0\\)\n\\(X_2=1\\)\n\n\n\n\n\\(X_1=0\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(X_1=1\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\\(X_2=2\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\n\n\nDonde la celda de la primer fila y primer columna se lee \\(P(X_1=0, X_2=0)=0.15\\)\n\nObtenga la distribuci√≥n marginal de \\(X_1\\).\nObtenga la distribuci√≥n marginal de \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_2\\) dado \\(X_1\\). \n\nSean \\(X_1\\) y \\(X_2\\) tales que \\((X_1, X_2)\\) siguen una distribuci√≥n normal bivariada con \\(\\mathbb{E}(X_1) = \\mathbb{E}(X_1)\\) = 0, \\(\\text{Var}(X_1) = \\text{Var}(X_2 = 1)\\) y \\(\\text{cor}(X_1, X_2) = \\rho\\)\n\nEncuentre la distribuci√≥n marginal de \\(X_1\\).\nEncuentre la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\)."
  },
  {
    "objectID": "practica/practica_00.html#esperanza-y-varianza",
    "href": "practica/practica_00.html#esperanza-y-varianza",
    "title": "Pr√°ctica - Unidad 0",
    "section": "Esperanza y Varianza",
    "text": "Esperanza y Varianza\n\nSuponga una urna \\(S\\) contiene un 40% de bolas verdes y un 60% de bolas rojas, y otra urna \\(E\\) contiene un 60% de bolas verdes y un 40% de bolas rojas. Una persona arroja una moneda de un peso argentino y selecciona una bola de una de las dos urnas dependiendo de si la moneda en sol o escudo. Si la moneda cae en sol, saca una bola de la urna \\(S\\) y si la moneda cae en escudo, saca una bola de la urna \\(E\\).\nConsidere las siguientes variables aleatorias:\n\\[\n\\begin{aligned}\nX &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en sol} \\\\\n    0 & \\text{Si la moneda cae en escudo}\n    \\end{cases}\n\\\\\n\\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la bola es verde} \\\\\n    0 & \\text{Si la bola es roja}\n    \\end{cases}\n\\end{aligned}\n\\]\n\nEncuentre la distribuci√≥n conjunta de \\(X\\) e \\(Y\\) en una tabla.\nEncuentre \\(\\mathbb{E}(Y)\\). ¬øCu√°l es la probabilidad de que la bola sea verde?\nEncuentre \\(\\text{Var}(Y | X = 0)\\), \\(\\text{Var}(Y | X = 1)\\) Y \\(\\text{Var}(Y)\\). Considerando a la varianza como una medida de incertidumbre, explique de manera intuitiva por que algunas variancias son mas grandes que otras.\nSuponga que observa que la bola es verde. ¬øCu√°l es la probabilidad de que la moneda haya caido en escudo? \n\n\n\n\n\n\nMoneda de un peso argentino acu√±ada en 1995"
  },
  {
    "objectID": "practica/practica_02.html",
    "href": "practica/practica_02.html",
    "title": "Pr√°ctica - Unidad 2",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "href": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Modelos de Distribuciones Conjugadas",
    "text": "Modelos de Distribuciones Conjugadas\n\nEn un campamento de verano para infantes se realizaron actividades que promueven el contacto con la naturaleza. Una de las tareas consisti√≥ en germinar semillas de tomate. Josefina plant√≥ 18 semillas en su almaciguera. Al cabo de 5 d√≠as, germinaron 8 de ellas. Llamaremos con \\(\\theta\\) a la probabilidad de que una semilla de tomate germine y asumimos una distribuci√≥n \\(\\text{Beta}(1, 1)\\).\n\nCalcular la media y el desv√≠o est√°ndar a posteriori de \\(\\theta\\) a mano.\nVerifique el c√°lculo utilizando R.\nCalcule un intervalo de credibilidad del 95% para \\(\\theta\\).\n\nA mano\nUsando R\n\n\n\n\n\n\n\nFoto de Markus Spiske en Unsplash\n\n\n\n\nEn la final del 2018 de la Copa del Mundo de la FIFA, Francia le gan√≥ a Croacia por 4 a 2. En funci√≥n de este resultado,\n\n¬øQu√© probabilidad hay de que Francia fuera un mejor equipo que Croacia?\nSi el mismo partido se jugara de nuevo (cosa que los franceses en aquella oportunidad no pidieron), ¬øcu√°l es la probabilidad de que Francia ganara de nuevo? \n\nDurante el desarrollo de las vacunas contra el COVID-19, un medio anunci√≥ para una determinada vacuna una eficacia del 100%.\n\nEn la fase 3 de un ensayo en adolescentes de entre 12 y 15 a√±os, la vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostr√≥ una eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo cl√≠nico involucr√≥ 2260 j√≥venes estadounidenses. En el ensayo, 18 casos de COVID-19 fueron observados en el grupo placebo (\\(n=1129\\)) y ninguno en el grupo vacunado (\\(n=1131\\))\n\nEs de esperar que, en un ensayo m√°s grande, aparezca alg√∫n caso de COVID-19 en el grupo que recibi√≥ el tratamiento. ¬øC√≥mo se estima la probabilidad de algo que a√∫n no ocurri√≥? \nLa regla del tres\nUna estudiante de Licenciatura en Estad√≠stica est√° releyendo su tesina antes de entregarla. Si en 20 p√°ginas encontrara 5 typos, ser√≠a razonable estimar \\(\\frac{5}{20} = \\frac{1}{4}\\) typos/p√°gina. ¬øPero qu√© ocurre si en 20 p√°ginas no encuentra ning√∫n error?\nVerifcar que, partiendo de un prior uniforme, \\(\\frac{3}{N}\\) es una estimaci√≥n razonable para \\(\\tau\\) (la tasa de typos por p√°gina), siendo \\(N\\) el n√∫mero de p√°ginas. Para ello, hallar la probabilidad de que $> \\(\\frac{3}{N}\\) para diferentes valores de \\(N\\).  4\nUna colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo producto al mismo precio. Un vendedor tiene 100% evaluaciones positivas, con 10 reviews. Otro tiene 96% de evaluaciones positivas, con 50 reviews. El √∫ltimo tiene 90% de comentarios positivos, con 200 evaluaciones. ¬øCu√°l de los tres vendedores le recomendar√≠as? \nInferencia sobre una distribuci√≥n de Poisson\nLa distribuci√≥n de masa de probabilidad Poisson se define como \\[\n\\begin{array}{lcr}\n\\displaystyle p(x | \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!} &\n\\text{con} &\nx \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\ndonde \\(\\lambda > 0\\) es la cantidad promedio de veces que ocurre el evento de inter√©s en un periodo o espacio determinado.\n\nDerive el estimador de m√°xima verosimilitud del par√°metro \\(\\lambda\\).\nDerive el posterior \\(p(\\lambda|D)\\) suponiendo que el prior sobre \\(\\lambda\\) es \\(p(\\lambda) = \\text{Ga}(\\lambda | a, b) \\propto \\lambda^{\\alpha-1}e^{-\\lambda b}\\). Ayuda: El posterior tambi√©n es una distribuci√≥n Gamma.\n¬øA qu√© valor tiende la media a posteriori cuando \\(a \\to 0\\) y \\(b \\to 0\\)? Recuerde que la media de una distribuci√≥n \\(\\text{Ga}(a, b)\\) es \\(a/b\\). \n\nInferencia sobre una distribuci√≥n Uniforme\nConsidere una distribuci√≥n uniforme crentrada en \\(0\\) y rango \\(2a\\). La funci√≥n de densidad de probabilidad es\n\\[\np(x) = \\frac{1}{2a}I(x \\in [-a, a])\n\\]\nSea \\(\\mathbf{X} = (X_1,..., X_n)\\) un vector de \\(n\\) variables aleatorias independientes e id√©nticamente distribuidas seg√∫n \\(p(x)\\)\nInferencia m√°ximo-veros√≠mil\n\n¬øCu√°l es el estimador m√°ximo veros√≠mil de \\(a\\) (ll√°melo \\(\\hat{a}\\))?\n¬øQu√© probabilidad le asigna el modelo a una nueva observaci√≥n \\(x_{n + 1}\\) usando \\(\\hat{a}\\)?\n¬øObserva alg√∫n problema con el resultado anterior? Si es as√≠, sugiera una alternativa mejor.\n\nInferencia Bayesiana\nEl prior conjugado de la distribuci√≥n uniforme es la distribuci√≥n de Pareto.\nSi \\(x \\sim \\text{Pareto}(x | \\alpha, m)\\), luego\n\\[\np(x| \\alpha, m) = \\frac{\\alpha m^\\alpha}{x^{\\alpha+1}} \\mathbb{I}(x \\ge m)\n\\]\nSi el prior es una distribuci√≥n de Pareto, la distribuci√≥n conjunta de \\(\\theta\\) y \\(\\mathbf{X} = (X_1,..., X_n)\\) es\n\\[\np(\\theta, \\mathbf{X})\n    = \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n    \\mathbb{I}(\\theta \\ge \\text{max}(\\mathbf{X}))\n\\]\nLlamando \\(M_x = \\text{max}(\\mathbf{X})\\). La evidencia (la probabilidad que las \\(n\\) muestras provengan de la misma distribuci√≥n uniforme) es\n\\[\n\\begin{aligned}\np(\\mathbf{X}) &= \\int_{M_x}^\\infty\n                 \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n                 d\\theta \\\\\n&=  \\begin{cases}\n    \\frac{\\alpha}{(n+\\alpha)m^n} & \\text{Si } M_x \\le m \\\\\n    \\frac{\\alpha m^\\alpha}{(n+\\alpha)m^{n+\\alpha}} & \\text{Si } M_x > m \\\\\n    \\end{cases}\n\\end{aligned}\n\\]\nDerive el posterior y muestre que puede ser expresado como una distribuci√≥n de Pareto.  \nInferencia sobre una distribuci√≥n Exponencial\nEl tiempo de vida de una m√°quina en a√±os \\(X\\) es modelado con una distribuci√≥n exponencial con par√°metro \\(\\theta\\) desconocido. La funci√≥n de densidad es\n\\[\n\\begin{array}{lcrr}\np(x | \\theta) = \\theta e^{-\\theta x} & \\text{con} & x \\ge 0, & \\theta \\ge 0\n\\end{array}\n\\]\n\nMuestre que el estimador m√°ximo veros√≠mil es \\(\\hat{\\theta} = 1/\\bar{x}\\)\n\nSuponga que se observan los siguientes tiempos de vida de tres m√°quinas independientes \\(x_1 = 5\\), \\(x_2 = 6\\), \\(x_3 = 4\\). ¬øCu√°l es el valor del estimador MV?\nUna experta del √°rea sugiere que \\(\\theta\\) debe tener una distribuci√≥n a priori que tambi√©n sea exponencial. \\[\n\\begin{aligned}\n\\theta &\\sim \\text{Exp}(\\lambda) \\\\\np(\\theta | \\lambda) &= \\lambda e^{-\\lambda \\theta}\n\\end{aligned}\n\\] Elija un valor para la distribuci√≥n a priori, ll√°melo \\(\\hat{\\lambda}\\), tal que \\(\\mathbb{E}(\\theta) = 1/3\\).\n¬øCu√°l es el posterior \\(p(\\theta | \\mathbf{X}, \\hat{\\lambda})\\)?\n¬øEs la distribuci√≥n exponencial conjugada con un likelihood exponencial?\nEncuentre la media del posterior, \\(\\mathbb{E}(\\theta | \\mathbf{X}, \\hat{\\lambda})\\)\nExplique por que difieren el estimador MV de la media a posteriori.\n¬øCu√°l es m√°s razonable en este ejemplo? \n\nCalculo de conjugada no tradicional\n\nBinomial negativa - Beta\nExponencial - Gamma. Modificar el ejercicio sobre Exponencial-Exponencial?\n\n7.4.5 de Reich"
  },
  {
    "objectID": "practica/practica_02.html#simulaciones",
    "href": "practica/practica_02.html#simulaciones",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Simulaciones",
    "text": "Simulaciones\n\nEl tiempo que un empleado de recursos humanos demora en hacer una entrevista tiene distribuci√≥n exponencial con media 30 minutos. Los tiempos de duraci√≥n de cada entrevista se pueden considerar independientes entre s√≠. Las entrevistas a postulantes para un trabajo est√°n programadas cada 15 minutos, comenzando desde las 8. Es v√°lido considerar que todos los postulantes llegan puntuales a su entrevista. Cuando la persona del turno de las 8:15 llega a la oficina\n\n¬øCu√°l es la probabilidad de que tenga que esperar antes de ser entrevistada?\n¬øCu√°l es el horario esperado al que terminar√° su entrevista? \n\nDos personas se conocen en la fila de embarque para un vuelo en un avi√≥n Airbus A330-300\n\n¬øCu√°l es la probabilidad de que tengan asientos en la misma fila?\n¬øCu√°l es la probabilidad de que est√©n sentados en asientos adyacentes?\n\nEl Problema de Monty Hall es trabajado con frecuencia en cursos de Probabilidad. Mediante simulaci√≥n, determine cu√°l es la probabilidad de ganar el juego si se cambia de puerta. To Do Explicar el juego. \n\n\n\n\n\nLas 3 puertas del problema de Monty Hall\n\n\n\n\nBas√°ndose en el siguiente tuit y conociendo el problema del cumplea√±os (¬øcu√°ntas personas debe haber en una habitaci√≥n para que la probabilidad de que dos de ellas cumplan a√±os el mismo d√≠a sea mayor a X%?) construir un gr√°fico similar al del tuit donde se grafique la probabilidad de que haya \\(n\\) personas que cumplan a√±os el mismo d√≠a para \\(K\\) personas presentes en la habitaci√≥n.\nEl √°lbum oficial del Mundial de F√∫tbol de Qatar 2022 consta de 638 figuritas. Cada paquete trae cinco figuritas.\n\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de tener a Messi?\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de sacar a Messi repetido?\n¬øCu√°ntos paquetes se necesitan, en promedio, para completar el √°lbum?\nSi a una persona le faltan diez figuritas para completar el √°lbum, ¬øcu√°ntos paquetes tiene que comprar para asegurarse de lograrlo?\n\nSi se arroja una moneda \\(n\\) veces, ¬øcu√°l es la probabilidad de que no haya secuencias de \\(k\\) caras?\n¬øCu√°l es la probabilidad de que tres personas en un ascensor con doce pisos presionen para ir a tres pisos consecutivos?\nPrevio a la final de la Copa Am√©rica 2021, los jugadores de la Selecci√≥n Argentina se re√∫nen en la habitaci√≥n del hotel como se describe en este tuit.\n\n¬øCu√°l es la probabilidad de que un jugador adivine una de diez cartas?\n¬øCu√°l es la probabilidad de que tres de ellos adivinen una de diez cartas?\n\n7.4.2 de Reich\nQuiz√°s algo con tasa de falsos descubrimientos, p-values y su distribuci√≥n‚Ä¶"
  },
  {
    "objectID": "practica/practica_02.html#elecci√≥n-de-distribuciones-a-priori",
    "href": "practica/practica_02.html#elecci√≥n-de-distribuciones-a-priori",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Elecci√≥n de distribuciones a priori?",
    "text": "Elecci√≥n de distribuciones a priori?\n\nEsbozar la distribuci√≥n de las siguientes variables\n\nEl n√∫mero de personas que compran caf√© en el bar de la facultad asumiendo distribuci√≥n de Poisson.\nEl peso de perros adultos en kilogramos asumiendo una distribuci√≥n Uniforme.\nEl peso de elefantes adultos en kilogramos asumiendo una distribuci√≥n Normal.\nEl peso de humanos adultos en libras asumiendo una distribuci√≥n asim√©trica hacia la derecha.\n\nPara cada uno cada uno de los ejemplos del ejercicio anterior, graficar la distribuci√≥n usando R. Seleccionar los par√°metros que creas razonable, tomar una muestra aleatoria de tama√±o 1000 y graficar la distribuci√≥n en base a las muestras. ¬øSe refleja tu conocimiento del problema en la distribuci√≥n graficada? Si no, ajustar los par√°metros y repetir el proceso hasta que el resultado tenga concuerde con el conocimiento del problema.\nComparar las siguientes distribuciones a priori.\n\n\\(\\text{Beta}(0.5, 0.5)\\)\n\\(\\text{Beta}(1, 1)\\)\n\\(\\text{Beta}(1, 1)\\)\n\\(\\text{Beta}(1, 4)\\)\n\\(\\text{Beta}(5, 1.5)\\)\n\n\n¬øEn qu√© se diferencian?\n¬øCu√°l de ellas es m√°s informativa?\n¬øC√≥mo lo determinaste?\n\nEn cada una de la situaciones que se describen debajo, ajustar manualmente los par√°metros de una distribuci√≥n \\(\\text{Beta}\\) para que reflejen la informaci√≥n brindada. En algunos casos puede haber varias respuestas aceptables en vez de existir ‚Äúla respuesta correcta‚Äù.\n\nUn amigo se postul√≥ para un empleo en LinkedIn y te dijo: ‚ÄúDir√≠a que tengo una chance del 40% de que me den el trabajo, pero no estoy seguro‚Äù. Cuando le preguntamos un poco mas, dijo que estima sus chances entre un 20% y un 60%.\nUn grupo de investigaci√≥n del CONICET desarroll√≥ una nueva prueba para una enfermedad bastante rara. El grupo espera que esta prueba arroje resultados correctos el 80% de las veces, con una varianza de 0.05.\nEl primo de un amigo disfruta de salir a pescar, y lo hace muy seguido. En el asado de los Jueves dice que ‚ÄúEl 90% de las veces que salgo a pescar, vuelvo con algo. Si te tuviera que dar un rango, te dir√≠a entre el 85% y el 100% de las veces‚Äù.\n\nEfecto de la parametrizaci√≥n\nSea \\(\\theta\\) la probabilidad de √©xito en un experimento binomial y sea \\(\\gamma = \\frac{\\theta}{1-\\theta}\\) la chance de √©xito. Utilizar simulaciones para explorar los efectos de las siguientes elecciones de distribuciones a priori\n\nSi \\(\\theta \\sim \\text{Uniforme}(0,1)\\), ¬øcu√°l es el prior inducido para \\(\\gamma\\)?\nSi \\(\\theta \\sim \\text{Beta}(0,1)\\), ¬øcu√°l es el prior inducido para \\(\\gamma\\)?\nSi \\(\\gamma \\sim \\text{Uniforme}(0,100)\\), ¬øcu√°l es el prior inducido para \\(\\theta\\)?\nSi \\(\\gamma \\sim \\text{Gamma}(1,1)\\), ¬øcu√°l es el prior inducido para \\(\\theta\\)?"
  },
  {
    "objectID": "practica/practica_02.html#notas",
    "href": "practica/practica_02.html#notas",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Notas",
    "text": "Notas\nEn alg√∫n lado podr√≠amos tener un glosario, o algo del estilo. El objetivo es despejar dudas, por ejemplo, sobre las parametrizaciones de las distribuciones que utilizamos por defecto. Ac√° podriamos mostrar que usamos \\(\\text{Beta}(a, b)\\) con \\(\\text{pdf}(x) = \\frac{\\Gamma(a + b)}{\\Gamma(a) + \\Gamma(b)} x^{a-1}(1-x)^{b-1}\\)"
  },
  {
    "objectID": "practica/practica_05.html",
    "href": "practica/practica_05.html",
    "title": "Pr√°ctica - Unidad 5",
    "section": "",
    "text": "Descargar PDF"
  },
  {
    "objectID": "practica/practica_05.html#section",
    "href": "practica/practica_05.html#section",
    "title": "Pr√°ctica - Unidad 5",
    "section": "",
    "text": "1. 4.6.3 de Reich\n2. 4.6.4 de Reich\n3. 4.6.9 de Reich\n4. 6.3 de Reich\n5. 5.7.1 de Reich"
  },
  {
    "objectID": "info/bibliografia.html",
    "href": "info/bibliografia.html",
    "title": "Bibliograf√≠a",
    "section": "",
    "text": "Bibliograf√≠a principal\n\nJohnson, Ott, y Dogucu (2022) McElreath (2020) Gelman y Hill (2006) Kruschke (2014) Reich y Ghosh (2019)\n\n\n\nGelman, Andrew, y Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel-Hierarchical Models. 1st edition. Cambridge University Press.\n\n\nJohnson, Alicia A., Miles Q. Ott, y Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling. 1st edition. Chapman; Hall/CRC. https://www.bayesrulesbook.com/.\n\n\nKruschke, John. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. 2nd edition. Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd edition. Chapman; Hall/CRC.\n\n\nReich, Brian J., y Sujit K. Ghosh. 2019. Bayesian Statistical Methods. 1st edition. Chapman; Hall/CRC.\n\n\n\n\nBibliograf√≠a complementaria\n\nGelman et¬†al. (2013), Gelman, Hill, y Vehtari (2021), Downey (2021), Lee y Wagenmakers (2014), Davidson-Pilon (2015), Nicenboim, Schad, y Vasishth (2022), Barr (2021), Carlin y Louis (2008), Hoff (2009), MacKay (2003), Lambert (2018), Murphy (2022), Murphy (2023), Bishop (2006), Martin, Kumar, y Lao (2021), Theoridis (2020), Clyde et¬†al. (2022), Ma, Kording, y Goldreich (2022)\n\n\n\n\n\nBarr, Dale J. 2021. Learning statistical models through simulation in R: An interactive textbook. 1st edition. https://psyteachr.github.io/stat-models-v1/.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. 1st edition. Springer.\n\n\nCarlin, Bradley P., y Thomas A. Louis. 2008. Bayesian Methods for Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nClyde, Merlise, Mine √áetinkaya-Rundel, Colin Rundel, David Banks, Christine Chai, y Lizzy Huang. 2022. An Introduction to Bayesian Thinking. 1st edition. https://statswithr.github.io/book/.\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. 1st edition. Addison-Wesley Data; Analytics Series.\n\n\nDowney, Allen B. 2021. Think Bayes: Bayesian Statistics in Python. 2nd edition. O‚ÄôReilly Media. http://allendowney.github.io/ThinkBayes2/.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, y Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nGelman, Andrew, Jennifer Hill, y Aki Vehtari. 2021. Regression and Other Stories. 1st edition. Cambridge University Press. https://users.aalto.fi/~ave/ROS.pdf.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. 1st edition. Springer.\n\n\nLambert, Ben. 2018. A Student‚Äôs Guide to Bayesian Statistics. 1st edition. SAGE Publications Ltd.\n\n\nLee, Michael D., y Eric-Jan Wagenmakers. 2014. Bayesian Cognitive Modeling: A Practical Course. 1st edition. Cambridge University Press.\n\n\nMa, Wei Ji, Konrad P. Kording, y Daniel Goldreich. 2022. Bayesian Models of Perception and Action: An Introduction. 3rd edition. http://www.cns.nyu.edu/malab/bayesianbook.html.\n\n\nMacKay, David J. C. 2003. Information Theory, Inference and Learning Algorithms. 1st edition. Cambridge University Press.\n\n\nMartin, Osvaldo A., Ravin Kumar, y Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. 1st edition. Chapman; Hall/CRC.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. 1st edition. The MIT Press. https://probml.ai/.\n\n\nMurphy, Kevin P. 2023. Probabilistic Machine Learning: Advanced Topics. 1st edition. The MIT Press. https://probml.ai/.\n\n\nNicenboim, Bruno, Daniel Schad, y Shravan Vasishth. 2022. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/.\n\n\nTheoridis, Sergios. 2020. Machine Learning: A Bayesian and Optimization Perspective. 2nd edition. Academic Press."
  },
  {
    "objectID": "info/enlaces_utiles.html",
    "href": "info/enlaces_utiles.html",
    "title": "Enlaces √∫tiles",
    "section": "",
    "text": "Awesome Bayesian Statistics. Es un listado de recursos en l√≠nea (y gratuitos!) relacionados al mundo de la Estad√≠stica Bayesiana."
  },
  {
    "objectID": "info/aprobacion.html",
    "href": "info/aprobacion.html",
    "title": "Condiciones de aprobaci√≥n",
    "section": "",
    "text": "Instancias de evaluaci√≥n\n\nüìù Parcial ‚Äì escrito e individual, se aprueba con 6, hay una instancia de recuperaci√≥n;\nüíªüíªüíª Trabajos pr√°cticos cortos ‚Äì grupales (hasta tres personas), se hacen por fuera del horario de clase, se entrega informe;\nüìà Trabajo pr√°ctico final ‚Äì realizaci√≥n grupal, defensa individual.\n\n\n\nCondiciones de aprobaci√≥n\n\nPromoci√≥n\n\nLas y los estudiantes que hayan aprobado el parcial o su recuperatorio (con nota \\(P\\)), los tres trabajos pr√°cticos cortos (con promedio simple \\(T\\)) y hayan entregado el trabajo pr√°ctico final, acceder√°n a una instancia de evaluaci√≥n oral donde se discutir√° el trabajo pr√°ctico final y se evaluar√°n de manera general todos los contenidos conceptuales de la asignatura; esta instancia puede incluir la realizaci√≥n de algunas actividades en computadora (el trabajo pr√°ctico final y la instancia oral tendr√°n nota \\(O\\)).\nLa nota final se obtendr√° seg√∫n \\(0.3\\ T + 0.4\\ P + 0.3\\ O\\)\n\nRegularidad\n\nQuienes no alcancen la condici√≥n de promoci√≥n podr√°n quedar en condici√≥n de estudiante regular si aprueban el parcial o su recuperatorio y aprueban al menos dos de los trabajos pr√°cticos cortos. Para alcanzar la aprobaci√≥n de la asignatura, los y las estudiantes en condici√≥n de regulares deber√°n presentar el trabajo final en una mesa de examen y aprobar una instancia oral de defensa del trabajo y de evaluaci√≥n integral de los contenidos de la materia. Esta instancia puede incluir la realizaci√≥n de algunas actividades en computadora.\n\nLibres\n\nAquellas personas que no alcancen la promoci√≥n de la asignatura ni la condici√≥n de estudiante regular quedar√°n en condici√≥n de estudiante libre. Los y las estudiantes en condici√≥n de libre deber√°n presentar un trabajo pr√°ctico y rendir un examen te√≥rico-pr√°ctico sobre la totalidad de los temas de la asignatura."
  },
  {
    "objectID": "info/faq.html",
    "href": "info/faq.html",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Preguntas frecuentes"
  },
  {
    "objectID": "info/docentes.html",
    "href": "info/docentes.html",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Docentes"
  },
  {
    "objectID": "info/programa.html",
    "href": "info/programa.html",
    "title": "Programa",
    "section": "",
    "text": "Fundamentaci√≥n\nLa Estad√≠stica Bayesiana es un enfoque de la inferencia estad√≠stica que se basa en utilizar probabilidades para representar el conocimiento disponible sobre el conjunto de par√°metros de un modelo y actualizar esa informaci√≥n utilizando la Regla de Bayes a partir de la observaci√≥n de un conjunto de datos. El conocimiento inicial se representa con una distribuci√≥n de probabilidad a priori, la informaci√≥n contenida en los datos observados se modeliza con una funci√≥n de verosimilitud y ambas fuentes de informaci√≥n se combinan para obtener una distribuci√≥n de probabilidad a posteriori. La informaci√≥n a posteriori puede ser utilizada para extraer conclusiones sobre el fen√≥meno en estudio y realizar predicciones sobre datos no observados o eventos futuros.\nLos m√©todos bayesianos requieren, salvo en casos muy simples, una complejidad computacional que resultaba inalcanzable hace algunos a√±os. Gracias a desarrollos revolucionarios en el √°mbito de la computaci√≥n, la principal barrera para la implementaci√≥n de los modelos bayesianos desapareci√≥ y la utilizaci√≥n de estos se ha incrementado masivamente en m√∫ltiples campos cient√≠ficos. Esta creciente popularidad se debe a que la inferencia bayesiana brinda un marco te√≥rico consistente que permite la incorporaci√≥n de informaci√≥n a priori, el desarrollo de un aprendizaje secuencial, la obtenci√≥n de inferencias y predicciones en forma de distribuciones de probabilidad, el tratamiento de datos faltantes, los an√°lisis con pocos datos, entre otras ventajas.\n\n\nObjetivos\nQue quienes cursen la materia logren:\n\nentender las caracter√≠sticas y los conceptos fundamentales de la Estad√≠stica Bayesiana;\ndescribir las caracter√≠sticas principales de la Estad√≠stica Bayesiana;\ncomprender la complejidad anal√≠tica de la inferencia bayesiana y la necesidad de la utilizaci√≥n de un enfoque computacional para superar estas dificultades;\nser capaces de aplicar m√©todos bayesianos a problemas reales utilizando software espec√≠fico; e\ninterpretar los resultados del proceso de an√°lisis bayesiano de datos.\n\n\n\nContenidos\n\nUnidad 1: Introducci√≥n y Fundamentos de la Estad√≠stica Bayesiana\n\nProbabilidad para cuantificar la incertidumbre. Modelos de probabilidad. Regla de Bayes. Inferencia bayesiana. Distribuci√≥n a priori, funci√≥n de verosimilitud, distribuci√≥n a posteriori.\n\nUnidad 2: Inferencia Bayesiana\n\nModelos de distribuciones conjugadas. Modelos de un par√°metro. Modelo beta-binomial. Enfoque intuitivo. Distribuci√≥n a posteriori como compromiso entre la verosimilitud y la distribuci√≥n a priori. Razonamiento secuencial. Modelo normal-normal. Modelo gamma‚ÄìPoisson. Modelos de varios par√°metros. Modelo normal ‚Äì normal-gamma-inversa. Modelo Dirichlet‚Äìmultinomial.\nElecci√≥n de distribuciones a priori: no informativas (impropias, de Jeffrey) y d√©bilmente informativas. Medidas de resumen de la distribuci√≥n a posteriori. Intervalos de credibilidad. Distribuci√≥n predictiva a posteriori. Nociones de teor√≠a de la decisi√≥n bayesiana. Riesgo bayesiano. Estimador de Bayes.\n\nUnidad 3: M√©todos Computacionales\n\nLimitaciones del enfoque anal√≠tico: c√°lculo de probabilidades y determinaci√≥n de la distribuci√≥n a posteriori. Soluciones: an√°lisis de datos simulados y aproximaci√≥n de grilla. Introducci√≥n al c√≥mputo bayesiano. Nociones b√°sicas de m√©todos de cadenas de Markov ‚Äì Montecarlo (MCMC). Algoritmo de Metropolis‚ÄìHastings. Montecarlo Hamiltoniano. Diagn√≥stico de m√©todos MCMC.\nProgramaci√≥n probabil√≠stica. Alternativas. Sintaxis de modelos. Ejemplos. Diagn√≥stico. Medidas de resumen a partir de las cadenas obtenidas. Visualizaciones.\n\nUnidad 4: Modelos Lineales\n\nModelos lineales. Elecci√≥n de distribuciones a priori. Regularizaci√≥n. Diagn√≥stico de modelos. Predicciones basadas en distribuciones de probabilidad. Pruebas predictivas a priori y a posteriori. Densidad predictiva a posteriori logar√≠tmica evaluada punto a punto (lppd). Deviance. Criterios de informaci√≥n: AIC, BIC, WAIC. Validacion cruzada. Sobreajuste y subajuste. Validaci√≥n cruzada utilizando muestreo por importancia mediante suavizado Pareto (PSIS-CV).\n\nUnidad 5: Modelos Avanzados\n\nRegresi√≥n log√≠stica. Regresi√≥n Poisson. Comparaci√≥n de grupos. Modelos de variable latente. Formulaci√≥n gr√°fica. An√°lisis de sensibilidad.\nEl enfoque multinivel: modelos jer√°rquicos. Modelo beta-binomial jer√°rquico. Shrinkage de par√°metros. Variaci√≥n en el intercepto. Variaci√≥n en la pendiente. Pooling de estimaciones. Problemas de estimaci√≥n."
  },
  {
    "objectID": "info/calendario.html",
    "href": "info/calendario.html",
    "title": "Calendario",
    "section": "",
    "text": "Semana\nFecha\nUnidad\nTemas\nApunte\nLectura sugerida\nOtras actividades\n\n\n\n\n1\n2023-03-20\n1\nProbabilidad para cuantificar la incertidumbre. Repaso de probabilidad. Modelos de probabilidad. Regla de Bayes.\n\n\n\n\n\n2\n2023-03-27\n1\nInferencia bayesiana. Distribucion a priori, funcion de verosimilitud y distribucion a posteriori.\n\n\n\n\n\n\n\n2\nConjugaci√≥n. Modelos conjugados a un par√°metro. Modelo beta-binomial. Posterior como compromismo entre verosimilitud y prior. Razonamiento secuencial.\n\n\n\n\n\n3\n2023-04-03\n2\nModelos conjugados a un par√°metro. Modelo normal-normal. Modelo gamma-poisson. Modelos de varios parametros. Normal-normal-gamma-inversa. Modelo Dirichlet-multinomial.\n\n\nPresentaci√≥n TP 1\n\n\n4\n2023-04-10\n2\nMedidas de resumen de la distribucion a posteriori. Distribucion predictiva a posteriori. Teoria de decision bayesiana. Estimador de Bayes.\n\n\n\n\n\n5\n2023-04-17\n3\nLimitaciones en el enfoque anal√≠tico. Aproximaci√≥n mediante grilla de puntos. Limitaciones en la aproximaci√≥n mediante grilla de puntos. Necesidad de contar con otro tipo de t√©cnicas. Soluci√≥n de problemas mediante t√©cnicas de simulaci√≥n. Obtenci√≥n de muestras de distribuciones de probabilidad.\n\n\nEntrega TP 1\n\n\n6\n2023-04-24\n3\nIntroducci√≥n al c√≥mputo bayesiano. Nociones b√°sicas de cadenas de Markov y de muestreo mediante el m√©todo de Algoritmo de Metr√≥polis-Hastings. Monte Carlo Hamiltoniano. Diagn√≥stico de m√©todos de MCMC. Monte Carlo basado en cadenas de Markov.\n\n\n\n\n\n7\n2023-05-01\n3\nProgramaci√≥n probabil√≠stica. Introducci√≥n a Stan. Sint√°xis b√°sica de modelos en Stan. Especificaci√≥n de modelos en Stan. Obtenci√≥n del posterior. Diagn√≥sticos. An√°lisis del posterior. Responder preguntas usando las muestras del posterior.\n\n\nParcialPresentaci√≥n TP 2\n\n\n8\n2023-05-08\n4\nRepaso de modelos lineales. Estimaci√≥n cl√°sica (MCO y MV). Limitaciones. Interpretaci√≥n frecuentista de los Intervalos de Confianza. Modelos lineales bajo el enfoque bayesiano. Uso de distribuciones a priori. Estimaci√≥n. Intervalos de Credibilidad y su interpretaci√≥n. Elecci√≥n de distribuciones a priori. Predicciones basadas en distribuciones de probabilidad. Distribuci√≥n predictiva a posteriori.\n\n\n\n\n\n9\n2023-05-15\n4\nModelos lineales. Introducci√≥n a brms. Reemplazo de Stan por brms. Diagn√≥sticos y selecci√≥n de modelos. lppd. Deviance. AIC, BIC, WAIC. Validaci√≥n cruzada. Sobreajuste y subajuste. PSIS-CV.\n\n\nEntrega TP 2Presentaci√≥n TP 3\n\n\n10\n2023-05-22\n4\nEjercitaci√≥n integradora de modelos lineales bayesianos usando R y brms.\n\n\n\n\n\n11\n2023-05-29\n5\nExtensi√≥n de los modelos lineales considerando funciones de verosimilitud no-normales. Introducci√≥n a MLG. Predictor lineal. Funci√≥n de enlace. Funci√≥n de verosimilitud. Menci√≥n a modelos para datos de conteo y para probabilidades. Modelos para datos de conteo. Verosimilitud Poisson. Funci√≥n de enlace logar√≠tmica, y otras funciones.\n\n\nEntrega TP 3\n\n\n12\n2023-06-05\n5\nEjercitaci√≥n con modelos para datos de conteo. Modelos para probabilidades. Verosimilitud Bernoulli. Funci√≥n de enlace logit. Interpretaci√≥n de coeficientes.\n\n\nPresentaci√≥n TP Final\n\n\n13\n2023-06-12\n5\n‚Ä¶\n\n\n\n\n\n14\n2023-06-19\n5\n‚Ä¶\n\n\n\n\n\n15\n2023-06-26\n5\n‚Ä¶\n\n\nDefensa oral del TP Final\n\n\n16\n2023-07-03\n5\n‚Ä¶"
  },
  {
    "objectID": "computo/instalacion.html#rstan",
    "href": "computo/instalacion.html#rstan",
    "title": "Instalaci√≥n de herramientas comutacionales",
    "section": "RStan",
    "text": "RStan\n\nWindows\n\n\nUbuntu\n\n\nMac OS\n\n\nVerificaci√≥n"
  },
  {
    "objectID": "computo/instalacion.html#librerias-adicionales",
    "href": "computo/instalacion.html#librerias-adicionales",
    "title": "Instalaci√≥n de herramientas comutacionales",
    "section": "Librerias adicionales",
    "text": "Librerias adicionales\n\n{brms}\n{ggplot2}\n{dplyr}\n**Estaria bueno pinear las dependencias usando {here} o algo similar\n\n\nVerificaci√≥n"
  },
  {
    "objectID": "computo/instalacion.html#dependencias-optativas",
    "href": "computo/instalacion.html#dependencias-optativas",
    "title": "Instalaci√≥n de herramientas comutacionales",
    "section": "Dependencias optativas",
    "text": "Dependencias optativas\n\nQuarto\n‚Ä¶ ?"
  },
  {
    "objectID": "complementario/variables.html",
    "href": "complementario/variables.html",
    "title": "Repaso de Variables Aleatorias",
    "section": "",
    "text": "Variables aleatorias\n\nDiscretas\nContinuas\n\nCDF\nPDF\nDistribucion conjunta\nDistribucion marginal\nDistribucion condicional\nIndependencia e independencia condicional\nMomentos de una distribucion\nRegla de Bayes\nDistribuciones de probabilidad frecuentemente utilizadas\nTransformacion de variables aleatorias\n\nPropiedades de transformaciones lineales\n\n\nEste listado esta sacado del capitulo 2 de Murphy (2022)\nTambien hay cosas interesantes en capitulo 3 de Murphy (2022)\n\nUncorrelated does not imply independent\nCorrelation does not imply causation\n\n\nPairwise independence does not imply mutual independence\nWe say that two random variables are pairwise independent if \\(p(X_2|X_1) = p(X_2)\\) and hence \\(p(X_2, X_1) = p(X_1)p(X_2|X_1) = p(X_1)p(X_2)\\)\nWe say that \\(n\\) random variables are mutually independent if \\(p(Xi|XS) = p(Xi)\\) \\(\\forall S \\subseteq \\{1, \\cdots , n\\}\\) and hence \\(\\displaystyle p(X_{1:n}) = \\prod_{i=1}^{n} p(X_i)\\)\nShow that pairwise independence between all pairs of variables does not necessarily imply mutual independence. It suffices to give a counter example.\nExercise 2.5 Murphy (2022)\nExercise 2.6 Murphy (2022)\nExercise 3.5 [Gaussian vs jointly Gaussian ]\n\n\n\n\n\nReferencias\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. 1st edition. The MIT Press. https://probml.ai/."
  }
]
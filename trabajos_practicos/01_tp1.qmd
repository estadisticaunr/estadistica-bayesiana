---
title: "TP1: The Multiarmed Bandit"
format:
    pdf:
        template: ../templates/template.tex
        template-partials:
            - ../templates/title.tex
        suppress-bibliography: true
    html: default
year: "2024"
course: "Estad칤stica Bayesiana"
practica: "Trabajo Pr치ctico 1"
logo: ../templates/logo.png
---

```{r}
#| echo: false
#| include: false

library(ggplot2)
library(patchwork)

pdf_file <- paste0(
    "https://github.com/estadisticaunr/estadistica-bayesiana/raw/pdf/trabajos_practicos/",
    paste0(sub("\\..*$", "", knitr::current_input()), ".pdf")
)
is_html <- knitr::is_html_output()
options("knitr.graphics.error" = FALSE)
captions <- list()
captions[["slotmachines2"]] <- "Tres maquinitas con diferentes probabilidades de ganar"
captions[["dirichlet"]] <- "Como los valores posibles del vector aleatorio tridimensional yacen en un plano, la distribuci칩n de probabilidad puede representarse gr치ficamente con facilidad "
```

::: {.content-visible when-format="html"}
[Descargar PDF](`r pdf_file`)
:::

# Introducci칩n

La vida nos enfrenta constantemente a decisiones que nos obligan a equilibrar entre la seguridad de lo familiar y la promesa de lo desconocido, un dilema conocido como *"explore vs. exploit"*. Esta dicotom칤a, se manifiesta en una multitud de escenarios cotidianos. Por ejemplo, consideremos la elecci칩n de una cafeter칤a para una merienda. 쯆ptamos por un lugar al que hemos ido muchas veces, conocido por su calidad constante (*'exploit'*), o probamos uno de los nuevos locales que abrieron en Rosario con el furor del caf칠 de especialidad que podr칤a ofrecer una experiencia culinaria incre칤ble o decepcionante (*'explore'*)? Esta elecci칩n representa una encrucijada entre lo seguro y lo novedoso, entre el confort de lo familiar y la emoci칩n de la novedad.

Este dilema tambi칠n se extiende a decisiones m치s significativas en nuestras vidas, como la elecci칩n de una carrera, donde *'exploit'* implicar칤a seguir en un campo donde ya tenemos habilidades y experiencia, mientras que *'explore'* nos llevar칤a a aventurarnos en un nuevo dominio, potencialmente m치s gratificante pero tambi칠n m치s arriesgado. Esta tensi칩n entre explorar y explotar no es solo una curiosidad te칩rica; es un principio fundamental que gu칤a nuestras decisiones diarias. Navegar entre estas dos opciones requiere una comprensi칩n profunda de nuestras metas, recursos y el entorno en el que operamos, y es una habilidad esencial para la adaptaci칩n y el 칠xito en un mundo en constante cambio.

Estos p치rrafos de *coaching emocional* sirven como la introducci칩n a este trabajo pr치ctico, donde estudiaremos el problema del _multi-armed bandit_, que pone 칠nfasis en el dilema *"explore vs. exploit"*. La traducci칩n de _multi-armed bandit_ es bandido multibrazo por lo que, por motivos obvios, nos quedaremos con la expresi칩n en ingl칠s.

# El _multi-armed bandit_

El _multi-armed bandit_ nos enfrenta a tres m치quinas tragamonedas, tragaperras (si es por usar traducciones poco felices) o simplemente maquinitas (como les decimos en Rosario) ...


```{r}
#| echo: false
#| out-width: 70%
#| fig-align: center
#| fig-cap: !expr captions[["cafe"]]
if (is_html) knitr::include_graphics(file.path("imgs", "slotsmachines2.png"))
```

Cada m치quina tiene una probabilidad de 칠xito desconocida y potencialmente diferente, es decir, una probabilidad distinta de entregar un premio. El desaf칤o consiste en decidir a cu치l m치quina dedicar nuestras tiradas con el objetivo de maximizar las ganancias totales. Aqu칤 es donde entra el dilema: 쯖onviene _"explotar"_ la m치quina que hasta ahora ha dado mejores resultados, o _"explorar"_ otras m치quinas que podr칤an tener una tasa de 칠xito mayor pero a칰n desconocida?

En la fase inicial, cuando se sabe poco sobre las m치quinas, podr칤a ser m치s prudente _"explorar"_, probando cada m치quina varias veces para obtener una estimaci칩n aproximada de sus probabilidades de 칠xito. A medida que se acumulan datos sobre el rendimiento de cada m치quina, la estrategia podr칤a cambiar a _"explotar"_ la m치quina que ha demostrado ser la m치s rentable. Sin embargo, siempre existe la incertidumbre y la posibilidad de que una de las m치quinas menos utilizadas tenga en realidad una tasa de 칠xito mayor. Este problema se complica a칰n m치s por el hecho de que cada elecci칩n de m치quina proporciona informaci칩n que podr칤a alterar nuestra comprensi칩n de cu치l es la mejor opci칩n. La soluci칩n 칩ptima a este problema involucra un equilibrio cuidadoso entre explorar para ganar informaci칩n y explotar esa informaci칩n para maximizar las ganancias.

En este trabajo pr치ctico consideraremos la situaci칩n simplificada e imaginaria en la que no cuesta dinero jugar con una m치quina. Es decir, si la m치quina da 칠xito, sumamos una unidad monetaria 游뱀, pero si no, no perdemos nada. Supondremos, adem치s, un escenario ficticio en que el deseo por descubrir cual es la m치quina ganadora nos tendr치 jugando los 366 d칤as del a침o 2024 游. Lo que s칤, cada d칤a jugaremos con una sola m치quina 游꿣 y volveremos al d칤a siguiente...

El objetivo del trabajo consiste en evaluar y comparar diferentes estrategias de juego. Se analizar치n mediante simulaciones diferentes estrategias de exploraci칩n y explotaci칩n de la m치quina. 쮻칩nde aparece la inferencia bayesiana? Partiremos de una creencia _a priori_ para la probabilidad de 칠xito de cada m치quina y la iremos actualizando con cada jugada. 

Para el estudio mediante simulaciones, consideraremos que las probabilidades de 칠xito de las tres m치quinas son $\theta_a = 0.30$, $\theta_b = 0.55$ y $\theta_c = 0.45$. Recordemos que estas probabilidades son desconocidas (no podemos basar nuestras estrategias en esos valores, sino en las estimaciones que vamos haciendo de ellos).

1. Simule 1000 repeticiones de una persona que tiene informaci칩n confidencial y privilegiada y juega 366 d칤as con la mejor m치quina. Realice un histograma del dinero acumulado al finalizar el per칤odo. 쮺u치nto se espera que gane en promedio?

# Estrategias 

Para todo lo que sigue de aqu칤 en adelante, consideremos que la creencia _a priori_ para $\theta_a$, $\theta_b$ y $\theta_c$ es $\mathrm{Beta}(2,2)$

## Completamente al azar

Considere la estrategia (o no-estrategia) m치s elemental: cada d칤a, elegir una m치quina al azar para jugar. 

1. Construya una funci칩n en R que elija una m치quina al azar, obtenga un resultado (칠xito o fracaso) y actualice la credibilidad sobre los posibles valores de la probabilidad de 칠xito correspondiente. 
2. Utilice esa funci칩n para simular una secuencia de 366 d칤as de juego. Registre la evoluci칩n diaria del dinero acumulado cada d칤a, la cantidad de veces que se juega en cada m치quina, y la distribuci칩n _a posteriori_ de cada probabilidad de 칠xito. Muestre gr치ficamente los resultados.
3. Simule 1000 secuencias de 366 d칤as de juego y analice los resultados.

## _Greedy_ con tasa observada

Se elige la m치quina que tenga la mayor tasa de 칠xito observada hasta el momento.

## _Greedy_ con probabilidad _a posteriori_

## _$\epsilon$-greedy_

Se selecciona la mejor m치quina (la de mayor tasa de 칠xito observada seg칰n los datos actuales) con una probabilidad de $1-\epsilon$ y se elige una m치quina al azar con una probabilidad $\epsilon$. 

## _Softmax_

Dada la tasa observada para cada m치quina $i$, $\pi_i$, se calcula una probabilidad de elegir cada m치quina utilizando la funci칩n _softmax_:

$$\mathrm{Pr}(i) = \frac{e^{\pi_i/\tau}}{\sum_{j=1}^3 e^{\pi_i/\tau}}$$
donde $\tau$ es un par치metro de "temperatura" que controla el grado de exploraci칩n (con valores m치s altos que promueven m치s exploraci칩n).

1. Implemente la funci칩n _softmax_ que, dadas tres tasas observadas, devuelva la probabilidad de elegir cada m치quina.

## _Upper-bound_

Se selecciona la m치quina que tenga el mayor extremo derecho de un intervalo de credibilidad (construido a partir de la distribuci칩n _a posteriori_)

## _Thompson sampling_




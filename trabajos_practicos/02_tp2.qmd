---
title: "TP2: Metropolis-Hastings"
practica: "Trabajo Pr谩ctico 2"
---

```{r}
#| echo: false
#| include: false
library(patchwork)
library(ggplot2)
library(dplyr)
```

# Metropolis-Hastings en 1D

El algoritmo de Metropolis-Hastings (MH) permite generar muestras (pseudo-)aleatorias a 
partir de una distribuci贸n de probabilidad $P$ que no necesariamente pertence a una 
familia de distribuciones conocida. El 煤nico requisito es que se pueda evaluar la funci贸n
de densidad (o de masa de probabilidad) $p^*(\theta)$ en cualquier valor de $\theta$,
incluso cuando $p^*(\theta)$ sea impropia (es decir, incluso aunque sea
desconocida la constante de normalizaci贸n que hace que la integral en el soporte de la 
funci贸n sea igual a uno).

1.  Escriba una funci贸n que implemente el algoritmo de Metropolis-Hastings para tomar 
    muestras de una distribuci贸n de probabilidad unidimensional a partir de su funci贸n de 
    densidad. Separe en funciones cada uno de los pasos del procedimiento. 
    Otorgue flexibilidad al algoritmo permitiendo elegir entre un punto de inicio arbitrario
    o al azar y utilizar distribuciones de propuesta de transici贸n arbitrarias 
    (por defecto, se utiliza una distribuci贸n normal est谩ndar).


La distribuci贸n de Kumaraswamy es una distribuci贸n de probabilidad continua que se utiliza
para modelar variables aleatorias con soporte en el intervalo $(0, 1)$. Si bien graficamente
la forma de su funci贸n de densidad puede hacernos recordar a la distribuci贸n beta, 
vale mencionar que la distribuci贸n de Kumaraswamy resulta en una expresi贸n matem谩tica
cuyo c贸mputo es m谩s sencillo:

$$
\begin{array}{lr}
p(x \mid a, b) = a b x ^ {a - 1} (1 - x ^ a)^{b - 1} & \text{con } a, b > 0
\end{array}
$$

2.  Grafique la funci贸n de densidad de la distribuci贸n de Kumaraswamy para 5 combinaciones
    de los par谩metros $a$ y $b$ que crea convenientes.

3.  Utilizando la funci贸n construida en el punto **1**, obtenga 5000 muestras de una 
    distribuci贸n de Kumaraswamy con par谩metros $a=6$ y $b=2$.
    Utilice una distribuci贸n de propuesta beta. Tenga en cuenta que la misma se puede 
    parametrizar seg煤n media $\mu = \alpha / (\alpha + \beta)$ y  concentraci贸n 
    $\kappa = \alpha + \beta$.

    Compare las cadenas obtenidas al utilizar tres diferentes grados de concentraci贸n
    en la distribuci贸n de propuesta. Calcule la tasa de aceptaci贸n. Compare utilizando 
    histogramas y funciones de autocorrelaci贸n (puede utilizar la funci贸n `acf` o 
    escribir una funci贸n propia). Para elegir el punto inicial del algoritmo de MH, 
    obtenga un valor aleatorio de una distribuci贸n conocida que crea conveniente.    
    
4.  Utilizando cada una de las cadenas anteriores, compute la media de la 
    distribuci贸n y los percentiles 5 y 95 de $X$ y de $\mathrm{logit}(X)$.

5.  Considere un experimento binomial a partir del cual se quiere determinar la 
    probabilidad de 茅xito $\theta$. Se realiza el experimento y se obtienen 8 茅xitos en 
    13 intentos. Obtenga la distribuci贸n _a posteriori_ de $\theta$ si la creencia 
    _a priori_ viene dada por

    $$
    p(\theta) = 2 \theta \qquad \theta \in (0, 1)
    $$

    Obtenga muestras utilizando 6 cadenas independientes que partan de diferentes puntos
    inciales. Estudie gr谩ficamente la convergencia y, en caso de ser necesario, descarte 
    muetras iniciales. Adem谩s, estime el tama帽o efectivo de muestra 
    ($ESS$ o $N_{\mathrm{eff}}$) y el MCSE de la media _a posteriori_ de la probabilidad 
    de 茅xito. Concluya sobre la bondad de la aproximaci贸n obtenida de la distribuci贸n 
    _a posteriori_ y su media.


## Metropolis-Hastings en 2D

Como veremos en esta secci贸n del trabajo pr谩ctico, la verdadera utilidad del algoritmo de
Metropolis-Hastings se aprecia cuando se obtienen muestras de distribuciones en m谩s de 
una dimensi贸n, incluso cuando no se conoce la constante de normalizaci贸n. 
Parad贸jicamente, los ejemplos trabajados a continuaci贸n tambi茅n ser谩n los que nos 
permitir谩n advertir sus limitaciones y motivar谩n la b煤squeda de mejores alternativas.

### Normal multivariada

La distribuci贸n normal multivariada es la generalizaci贸n de la distribuci贸n normal 
univariada a m煤ltiples dimensiones (mejor dicho, el caso en una dimensi贸n es un caso 
particular de la distribuci贸n en m煤ltiples dimensiones). La funci贸n de densidad de 
la distribuci贸n normal en $k$ dimensiones es:

$$
p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = 
    \frac{1}{(2\pi)^{k/2} |\boldsymbol{\Sigma}|^{1/2}} 
    \exp\left(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right)
$$

donde $\boldsymbol{\mu}$ es el vector de medias y $\boldsymbol{\Sigma}$ la matriz de covarianza.

6.  Escriba una funci贸n que implemente el algoritmo de Metropolis-Hastings para tomar 
    muestras de una funci贸n de probabilidad bivariada dada. Separe en funciones cada una 
    de los pasos del algoritmo. La probabilidad de salto ser谩 normal bivariada de matriz 
    de covarianza variable (utilizar para ello la funci贸n `rmvnorm` del paquete 
    `{mvtnorm}`). Otorgue flexibilidad al algoritmo haciendo que reciba como 
    argumento la matriz de covarianza de la probabilidad de transici贸n.

7.  Utilice la funci贸n escrita en el punto anterior para obtenga muestras de una 
    distribuci贸n normal bivariada con media $\boldsymbol{\mu}^*$ y matriz de covarianza 
    $\boldsymbol{\Sigma}^*$. Determine una matriz de covarianza que crea conveniente para la 
    distribuci贸n de propuesta. Justifique su decisi贸n y valide la bondad del m茅todo 
    mediante el uso de _traceplots_ y las estad铆sticas que crea adecuadas.
    
    $$
    \begin{array}{lr}
        \boldsymbol{\mu}^* = \begin{bmatrix} 0.4 \\ 0.75 \end{bmatrix} 
        & \boldsymbol{\Sigma}^* = \begin{bmatrix} 1.35 & 0.4 \\ 0.4 & 2.4 \end{bmatrix}
    \end{array}
    $$

8.  Estime las siguientes probabilidades utilizando las muestras:

    i.  $\mathrm{Pr}(X_1 > 1, X_2 < 0)$
    i.  $\mathrm{Pr}(X_1 > 1, X_2 > 2)$
    i.  $\mathrm{Pr}(X_1 > 0.4, X_2 > 0.75)$

    Luego, calcule esas mismas probabilidades mediante alg煤n m茅todo que crea conveniente 
    (funci贸n de distribuci贸n, integraci贸n manual, integraci贸n num茅rica, monte carlo, etc.),
    compare con las obtenidas con las muestras obtenidas con MH y concluya.


### Funci贸n de Rosenbrock 

La [funci贸n de Rosenbrock](https://es.wikipedia.org/wiki/Funci%C3%B3n_de_Rosenbrock), 
a veces llamada el "valle de Rosenbrock", y comunmente conocida como la 
"banana de Rosenbrock" , es una funci贸n matem谩tica utilizada frecuentemente como un 
problema de optimizaci贸n y prueba para algoritmos de optimizaci贸n num茅rica. 

La funci贸n est谩 definida por:
$$
f(x, y) = (a - x) ^ 2 + b(y - x^2) ^ 2
$$

y cuenta con un m铆nimo global en $(x, y) = (a, a^2)$, que satisface $f(a, a^2) = 0$.

Debido a su forma peculiar, la funci贸n de Rosenbrock presenta desaf铆os particulares para 
los algoritmos de optimizaci贸n, ya que tiene un valle largo y estrecho en el que la 
convergencia puede ser lenta.

```{r}
#| echo: false
#| out-width: 60%
#| fig-align: center
#| fig-cap: "Funci贸n de Rosenbrock"
knitr::include_graphics(file.path("imgs", "rosenbrock2.png"))
```

Esta forma de banana popularizada por Rosenbrock es tambi茅n muy conocida en el campo
de la estad铆stica bayesiana, ya que en ciertos escenarios, la densidad del _a posteriori_ 
toma una forma que definitivamente se asemeja a la banana de Rosenbrock. Un ejemplo de
este fen贸meno es la funci贸n $p^*$:

$$
p^*(x, y \mid a, b) = \exp \left[-\left((a - x) ^ 2 + b(y - x^2) ^ 2\right) \right]
$$


```{r}
#| echo: false
#| fig-align: center
#| fig-width: 8
#| fig-height: 5
#| fig-cap: Funci贸n de densidad de la que se desean obtener muestras con $a = 0.5$ y $b = 5$

f <- function(x, y) {
    a <- 0.5
    b <- 5
    exp(- ((a - x) ^ 2 + b * (y - x^2) ^ 2))
}

x1 <- seq(-2.5, 2.5, length.out = 100)
x2 <- seq(-1, 6, length.out = 100)

data <- tidyr::crossing(x1 = x1, x2 = x2)
data |>
    mutate(f = purrr::map2_dbl(x1, x2, ~ f(.x, .y))) |>
    ggplot() + 
    geom_raster(aes(x = x1, y = x2, fill = f)) +
    stat_contour(aes(x = x1, y = x2, z = f), col = "white", bins = 8) +
    # geom_hline(yintercept = seq(-1, 6, by = 1)) +
    # geom_vline(xintercept = seq(-2, 2), by = 0.25) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(x = "x", y = "y", fill = expression(p^"*" ~ "(x, y | a, b)")) +
    viridis::scale_fill_viridis()
```

9.  Mediante la funci贸n implementada en el punto **6** obtenga muestras de la 
    distribuci贸n _a posteriori_ determinada por $p^*$ con $a=0.5$ y $b=5$. 
    Proponga y utilice tres matrices de covarianza distintas para la distribuci贸n de 
    propuesta. Para al menos dos de los casos, compare las trayectorias seguidas por las 
    cadenas en el proceso de muestreo.  En todos los casos, calcule la probabilidad de
    aceptaci贸n y muestre la funci贸n de autocorrelaci贸n.

10. Con el conjunto de muestras que crea mas conveniente, estime las siguientes
    probabilidades:

    i.  $\mathrm{Pr}(0 < X_1 < 1, 0 < X_2 < 1)$
    i.  $\mathrm{Pr}(-1 < X_1 < 0, 0 < X_2 < 1)$
    i.  $\mathrm{Pr}(1 < X_1 < 2, 2 < X_2 < 3)$

11. Finalmente, calcule las mismas probabilidades utilizando el m茅todo que crea mas 
    conveniente (integraci贸n manual, integraci贸n num茅rica, m茅todo de la grilla, etc.) y
    concluya sobre los resultados en este experimento en particular y sobre la utilidad
    del algoritmo de Metropolis-Hastings en general.


<!--
# Ap茅ndice (para mover a teor铆a)

::: {.callout-tip}
## Algoritmo de Metropolis Hastings

Se desea generar una muestra de valores $\{y^{(1)}, y^{(2)}, \cdots, y^{(n)} \}$ a partir
de una distribuci贸n de probabilidad $P$ con funci贸n de densidad $p$.

1. Seleccionar un punto inicial $y^{(1)}$.

1. Para cada $t\in \{1, \cdots, n\}$, repetir:

    i.  **Proponer un nuevo valor**
    
        Obtener un valor aleatorio $y'$ de una variable $Y'$ cuya distribuci贸n est谩
        dada por la distribuci贸n de propuesta $Q$ y el valor de la 煤ltima muestra 
        obtenida:
    
        $$
        Y' \sim Q(y^{(t)})
        $$
    
    i.  **Calcular la probabilidad de aceptaci贸n**

        Calcular el cociente entre la funci贸n de densidad en el punto propuesto y en el
        punto actual. La probabilidad de aceptaci贸n es igual a este cociente si es menor 
        a 1, caso contrario es igual a 1.

        $$
        \alpha = \min \left\{ 1, \frac{p(y')}{p(y)} \right\}
        $$

    i.  **Seleccionar el nuevo valor**

        Generar un valor aleatorio $u$ de una distribuci贸n $\mathcal{U}(0, 1)$ y determinar
        $y^{(t + 1)}$ de la siguiente manera:

        $$
        y^{(t + 1)} = \left\{
        \begin{array}{ll}
        y' & \text{si} \quad u \le \alpha \\
        y^{(t)} & \text{si} \quad u > \alpha
        \end{array}\right.
        $$

**Notas**

El c谩lculo del cociente en la determinaci贸n de la probabilidad de aceptaci贸n es
en realidad:

$$
\frac{p(y')q(y^{(t)} \mid y')}{p(y)q(y' \mid y^{(t)})}
$$

donde $q$ es la funci贸n de densidad de la distribuci贸n de propuesta.

Esta se simplifica a la expresi贸n utilizada en el algoritmo cuando $q$ es una funci贸n 
sim茅trica alrededor de su media.
:::


::: {.callout-tip}
## _Effective sample size_ (ESS)

El n煤mero efectivo de muestras $N_{eff}$ es el n煤mero de muestras independientes que tienen el mismo _poder de estimaci贸n_ que $S$ muestras correlacionadas.

Este valor puede aproximarse por:

$$N_{eff} = \frac{S}{1 + 2 \sum_{k=1}^\infty ACF(k)}$$

Notar que la suma infinita del denominador empieza en $k=1$ (y no en $k=0$, donde $ACF(0)=1$). Adem谩s, en la pr谩ctica, una regla para truncar la $ACF$ es hacerlo a partir del primer $k$ valor donde $ACF(k)<0.05$ [@Kruschke2014, p. 184].
:::

::: {.callout-tip}
## _Montecarlo standard error_ (MCSE)

Por el Teorema Central del L铆mite sabemos que, si $\bar{X}_N$ es el promedio de $N$ observaciones 
independientes e id茅nticamente distribuidas, entonces $\sqrt{N}(\bar{X}_N-\mu)$ converge en distribuci贸n 
a $\mathcal{N}(0,\sigma^2)$ cuando $N$ tiende a infinito, donde $\mu$ es la media de la distribuci贸n
de las $X_i$ y $\sigma$ es su desv铆o est谩ndar. Si $\sigma$ se estima por $\hat{\sigma}$, el t茅rmino 
$\frac{\sigma}{\sqrt{N}}$ se conoce como error est谩ndar.

Cuando se realiza integraci贸n por Montecarlo y se estima $\mathbb{E}({X})$ con $N_{eff}$ muestras dependientes
que se comportan como $N$ muestras independientes, el error est谩ndar se aproxima por:
$$
MCSE = \frac{\hat{\sigma}}{\sqrt{N_{eff}}}
$$
:::

-->
